{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introductory applied machine learning\n",
    "# Assignment 3 (Part B): Mini-Challenge [25%]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important Instructions\n",
    "\n",
    "**It is important that you follow the instructions below to the letter - we will not be responsible for incorrect marking due to non-standard practices.**\n",
    "\n",
    "1. <font color='red'>We have split Assignment 3 into two parts to make it easier for you to work on them separately and for the markers to give you feedback. This is part B of Assignment 3 - Part A is an introduction to Object Recognition. Both Assignments together are still worth 50% of CourseWork 2. **Remember to submit both notebooks (you can submit them separately).**</font>\n",
    "\n",
    "1. You *MUST* have your environment set up as in the [README](https://github.com/michael-camilleri/IAML2018) and you *must activate this environment before running this notebook*:\n",
    "```\n",
    "source activate py3iaml\n",
    "cd [DIRECTORY CONTAINING GIT REPOSITORY]\n",
    "jupyter notebook\n",
    "# Navigate to this file\n",
    "```\n",
    "\n",
    "1. Read the instructions carefully, especially where asked to name variables with a specific name. Wherever you are required to produce code you should use code cells, otherwise you should use markdown cells to report results and explain answers. In most cases we indicate the nature of answer we are expecting (code/text), and also provide the code/markdown cell where to put it\n",
    "\n",
    "1. This part of the Assignment is the same for all students i.e. irrespective of whether you are taking the Level 10 version (INFR10069) or the Level-11 version of the course (INFR11182 and INFR11152).\n",
    "\n",
    "1. The .csv files that you will be using are located at `./datasets` (i.e. use the `datasets` directory **adjacent** to this file).\n",
    "\n",
    "1. In the textual answer, you are given a word-count limit of 600 words: exceeding this will lead to penalisation.\n",
    "\n",
    "1. Make sure to distinguish between **attributes** (columns of the data) and **features** (which typically refers only to the independent variables, i.e. excluding the target variables).\n",
    "\n",
    "1. Make sure to show **all** your code/working. \n",
    "\n",
    "1. Write readable code. While we do not expect you to follow [PEP8](https://www.python.org/dev/peps/pep-0008/) to the letter, the code should be adequately understandable, with plots/visualisations correctly labelled. **Do** use inline comments when doing something non-standard. When asked to present numerical values, make sure to represent real numbers in the appropriate precision to exemplify your answer. Marks *WILL* be deducted if the marker cannot understand your logic/results.\n",
    "\n",
    "1. **Collaboration:** You may discuss the assignment with your colleagues, provided that the writing that you submit is entirely your own. That is, you must NOT borrow actual text or code from others. We ask that you provide a list of the people who you've had discussions with (if any). Please refer to the [Academic Misconduct](http://web.inf.ed.ac.uk/infweb/admin/policies/academic-misconduct) page for what consistutes a breach of the above.\n",
    "\n",
    "\n",
    "### SUBMISSION Mechanics\n",
    "\n",
    "**IMPORTANT:** You must submit this assignment by **Thursday 15/11/2018 at 16:00**. \n",
    "\n",
    "**Late submissions:** The policy stated in the School of Informatics is that normally you will not be allowed to submit coursework late. See the [ITO webpage](http://web.inf.ed.ac.uk/infweb/student-services/ito/admin/coursework-projects/late-coursework-extension-requests) for exceptions to this, e.g. in case of serious medical illness or serious personal problems.\n",
    "\n",
    "**Resubmission:** If you submit your file(s) again, the previous submission is **overwritten**. We will mark the version that is in the submission folder at the deadline.\n",
    "\n",
    "**N.B.**: This Assignment requires submitting **two files (electronically as described below)**:\n",
    " 1. This Jupyter Notebook (Part B), *and*\n",
    " 1. The Jupyter Notebook for Part A\n",
    " \n",
    "All submissions happen electronically. To submit:\n",
    "\n",
    "1. Fill out this notebook (as well as Part A), making sure to:\n",
    "   1. save it with **all code/text and visualisations**: markers are NOT expected to run any cells,\n",
    "   1. keep the name of the file **UNCHANGED**, *and*\n",
    "   1. **keep the same structure**: retain the questions, **DO NOT** delete any cells and **avoid** adding unnecessary cells unless absolutely necessary, as this makes the job harder for the markers. This is especially important for the textual description and probability output (below).\n",
    "\n",
    "1. Submit it using the `submit` functionality. To do this, you must be on a DICE environment. Open a Terminal, and:\n",
    "   1. **On-Campus Students**: navigate to the location of this notebook and execute the following command:\n",
    "   \n",
    "      ```submit iaml cw2 03_A_ObjectRecognition.ipynb 03_B_MiniChallenge.ipynb```\n",
    "      \n",
    "   1. **Distance Learners:** These instructions also apply to those students who work on their own computer. First you need to copy your work onto DICE (so that you can use the `submit` command). For this, you can use `scp` or `rsync` (you may need to install these yourself). You can copy files to `student.ssh.inf.ed.ac.uk`, then ssh into it in order to submit. The following is an example. Replace entries in `[square brackets]` with your specific details: i.e. if your student number is for example s1234567, then `[YOUR USERNAME]` becomes `s1234567`.\n",
    "   \n",
    "    ```\n",
    "    scp -r [FULL PATH TO 03_A_ObjectRecognition.ipynb] [YOUR USERNAME]@student.ssh.inf.ed.ac.uk:03_A_ObjectRecognition.ipynb\n",
    "    scp -r [FULL PATH TO 03_B_MiniChallenge.ipynb] [YOUR USERNAME]@student.ssh.inf.ed.ac.uk:03_B_MiniChallenge.ipynb\n",
    "    ssh [YOUR USERNAME]@student.ssh.inf.ed.ac.uk\n",
    "    ssh student.login\n",
    "    submit iaml cw2 03_A_ObjectRecognition.ipynb 03_B_MiniChallenge.ipynb\n",
    "    ```\n",
    "    \n",
    "   What actually happens in the background is that your file is placed in a folder available to markers. If you submit a file with the same name into the same location, **it will *overwrite* your previous submission**. You should receive an automatic email confirmation after submission.\n",
    "  \n",
    "\n",
    "\n",
    "### Marking Breakdown\n",
    "\n",
    "The Level 10 and Level 11 points are marked out of different totals, however these are all normalised to 100%. Note that Part A (Object Recognition) is worth 75% of the total Mark for Assignment 3, while Part B (this notebook) is worth 25%. Keep this in mind when allocating time for this assignment.\n",
    "\n",
    "**70-100%** results/answer correct plus extra achievement at understanding or analysis of results. Clear explanations, evidence of creative or deeper thought will contribute to a higher grade.\n",
    "\n",
    "**60-69%** results/answer correct or nearly correct and well explained.\n",
    "\n",
    "**50-59%** results/answer in right direction but significant errors.\n",
    "\n",
    "**40-49%** some evidence that the student has gained some understanding, but not answered the questions\n",
    "properly.\n",
    "\n",
    "**0-39%** serious error or slack work.\n",
    "\n",
    "Note that while this is not a programming assignment, in questions which involve visualisation of results and/or long cold snippets, some marks may be deducted if the code is not adequately readable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Use the cell below to include any imports you deem necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Nice Formatting within Jupyter Notebook\n",
    "%matplotlib inline\n",
    "from IPython.display import display # Allows multiple displays from a single code-cell\n",
    "\n",
    "# System functionality\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "# Import Here any Additional modules you use. To import utilities we provide, use something like:\n",
    "#   from utils.plotter import plot_hinton\n",
    "import re\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix, log_loss\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import pylab\n",
    "pylab.rcParams['figure.figsize'] = (16, 9)\n",
    "\n",
    "# Your Code goes here:\n",
    "\n",
    "# Getting far too many warnings\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini challenge\n",
    "\n",
    "In this second part of the assignment we will have a mini object-recognition challenge. Using the same type of data as in Part A, you are asked to find the best classifier for the person/no person classification task. You can apply any preprocessing steps to the data that you think fit and employ any classifier you like (with the provision that you can explain what the classifier is/preprocessing steps are doing). You can also employ any lessons learnt during the course, either from previous Assignments, the Labs or the lecture material to try and squeeze out as much performance as you possibly can. The only restriction is that all steps must be performed in `Python` by using the `numpy`, `pandas` and `sklearn` packages. You can also make use of `matplotlib` and `seaborn` for visualisation.\n",
    "\n",
    "### DataSet Description\n",
    "\n",
    "The datasets we use here are similar in composition but not the same as the ones used in Part A: *it will be useful to revise the description in that notebook*. Specifically, you have access to three new datasets: a training set (`Images_C_Train.csv`), a validation set (`Images_C_Validate.csv`), and a test set (`Images_C_Test.csv`). You must use the former two for training and evaluating your models (as you see fit). As before, the full data-set has 520 attributes (dimensions). Of these you only have access to the 500 features (`dim1` through `dim500`) to test your model on: i.e. the test set does not have any of the class labels.\n",
    "\n",
    "### Model Evaluation\n",
    "\n",
    "Your results will be evaluated in terms of the logarithmic loss metric, specifically the [logloss](http://scikit-learn.org/0.19/modules/model_evaluation.html#log-loss) function from SKLearn. You should familiarise yourself with this. To estimate this metric you will need to provide probability outputs, as opposed to discrete predictions which we have used so far to compute classification accuracies. Most models in `sklearn` implement a `predict_proba()` method which returns the probabilities for each class. For instance, if your test set consists of `N` datapoints and there are `K` class-labels, the method will return an `N` x `K` matrix (with rows summing to 1).\n",
    "\n",
    "### Submission and Scoring\n",
    "\n",
    "This part of Assignment 3 carries 25% of the total marks. Within this, you will be scored on two criteria:\n",
    " 1. 80% of the mark will depend on the thoroughness of the exploration of various approaches. This will be assessed through your code, as well as a brief description (<600 words) justifying the approaches you considered, your exploration pattern and your suggested final approach (and why you chose it).\n",
    " 1. 20% of the mark will depend on the quality of your predictions: this will be evaluated based on the logarithmic loss metric.\n",
    "Note here that just getting exceptional performance is not enough: in fact, you should focus more on analysing your results that just getting the best score!\n",
    "\n",
    "You have to submit the following:\n",
    " 1. **All Code-Cells** which show your **working** with necessary output/plots already generated.\n",
    " 1. In **TEXT** cell `#ANSWER_TEXT#` you are to write your explanation (<600 words) as described above. Keep this brief and to the point. **Make sure** to keep the token `#ANSWER_TEXT#` as the first line of the cell!\n",
    " 1. In **CODE** cell `#ANSWER_PROB#` you are to submit your predictions. To do this:\n",
    "    1. Once you have chosen your favourite model (and pre-processing steps) apply it to the test-set and estimate the posterior proabilities for the data points in the test set.\n",
    "    1. Store these probabilities in a 2D numpy array named `pred_probabilities`, with predictions along the rows i.e. each row should be a complete probability distribution over whether the image contains a person or not. Note that due to the encoding of the `is_person` class, the negative case (i.e. there is no person) comes first.\n",
    "    1. Execute the `#ANSWER_PROB#` code cell, making sure to not change anything. This cell will do some checks to ensure that you are submitting the right shape of array.\n",
    "\n",
    "You may create as many code cells as you need (within reason) for training your models, evaluating the data etc: however, the text cell `#ANSWER_TEXT#` and code-cell `#ANSWER_PROB#` showing your answers must be the last two cells in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is where your working code should start. Fell free to add as many code-cells as necessary.\n",
    "#  Make sure however that all working code cells come BEFORE the #ANSWER_TEXT# and #ANSWER_PROB#\n",
    "#  cells below.\n",
    "\n",
    "# Your Code goes here:\n",
    "\n",
    "imagesC = pd.read_csv('datasets/Images_C_Train.csv')\n",
    "regex = re.compile('dim\\d*')\n",
    "keep = [col for col in imagesC.columns if regex.search(col)] + ['is_person']\n",
    "imTrain = imagesC[keep]\n",
    "imTest  = pd.read_csv('datasets/Images_C_Test.csv')#[keep] There are only dim data for the test set\n",
    "imVal   = pd.read_csv('datasets/Images_C_Validate.csv')[keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dim1</th>\n",
       "      <th>dim2</th>\n",
       "      <th>dim3</th>\n",
       "      <th>dim4</th>\n",
       "      <th>dim5</th>\n",
       "      <th>dim6</th>\n",
       "      <th>dim7</th>\n",
       "      <th>dim8</th>\n",
       "      <th>dim9</th>\n",
       "      <th>dim10</th>\n",
       "      <th>...</th>\n",
       "      <th>dim492</th>\n",
       "      <th>dim493</th>\n",
       "      <th>dim494</th>\n",
       "      <th>dim495</th>\n",
       "      <th>dim496</th>\n",
       "      <th>dim497</th>\n",
       "      <th>dim498</th>\n",
       "      <th>dim499</th>\n",
       "      <th>dim500</th>\n",
       "      <th>is_person</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2113.000000</td>\n",
       "      <td>2113.000000</td>\n",
       "      <td>2113.000000</td>\n",
       "      <td>2113.000000</td>\n",
       "      <td>2113.000000</td>\n",
       "      <td>2113.000000</td>\n",
       "      <td>2113.000000</td>\n",
       "      <td>2113.000000</td>\n",
       "      <td>2113.000000</td>\n",
       "      <td>2113.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2113.000000</td>\n",
       "      <td>2113.000000</td>\n",
       "      <td>2113.000000</td>\n",
       "      <td>2113.000000</td>\n",
       "      <td>2113.000000</td>\n",
       "      <td>2113.000000</td>\n",
       "      <td>2113.000000</td>\n",
       "      <td>2113.000000</td>\n",
       "      <td>2113.000000</td>\n",
       "      <td>2113.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.043353</td>\n",
       "      <td>0.050830</td>\n",
       "      <td>0.047988</td>\n",
       "      <td>0.037411</td>\n",
       "      <td>0.043365</td>\n",
       "      <td>0.049670</td>\n",
       "      <td>0.051071</td>\n",
       "      <td>0.043601</td>\n",
       "      <td>0.052698</td>\n",
       "      <td>0.054139</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052482</td>\n",
       "      <td>0.044370</td>\n",
       "      <td>0.046100</td>\n",
       "      <td>0.043218</td>\n",
       "      <td>0.049724</td>\n",
       "      <td>0.050818</td>\n",
       "      <td>0.038214</td>\n",
       "      <td>0.050150</td>\n",
       "      <td>0.051801</td>\n",
       "      <td>0.448178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.513260</td>\n",
       "      <td>0.582839</td>\n",
       "      <td>0.482166</td>\n",
       "      <td>0.464611</td>\n",
       "      <td>0.491187</td>\n",
       "      <td>0.558422</td>\n",
       "      <td>0.577599</td>\n",
       "      <td>0.472706</td>\n",
       "      <td>0.571629</td>\n",
       "      <td>0.588484</td>\n",
       "      <td>...</td>\n",
       "      <td>0.616955</td>\n",
       "      <td>0.466802</td>\n",
       "      <td>0.528117</td>\n",
       "      <td>0.508790</td>\n",
       "      <td>0.561206</td>\n",
       "      <td>0.562799</td>\n",
       "      <td>0.437885</td>\n",
       "      <td>0.581028</td>\n",
       "      <td>0.569857</td>\n",
       "      <td>0.497425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000868</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001359</td>\n",
       "      <td>0.000781</td>\n",
       "      <td>0.001116</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.000340</td>\n",
       "      <td>0.000651</td>\n",
       "      <td>0.000756</td>\n",
       "      <td>0.001116</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>0.001764</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.000744</td>\n",
       "      <td>0.001116</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.001616</td>\n",
       "      <td>0.000340</td>\n",
       "      <td>0.003516</td>\n",
       "      <td>0.001698</td>\n",
       "      <td>0.002038</td>\n",
       "      <td>0.001860</td>\n",
       "      <td>0.000756</td>\n",
       "      <td>0.001645</td>\n",
       "      <td>0.001698</td>\n",
       "      <td>0.002155</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002717</td>\n",
       "      <td>0.001359</td>\n",
       "      <td>0.003125</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.002038</td>\n",
       "      <td>0.002038</td>\n",
       "      <td>0.002038</td>\n",
       "      <td>0.002268</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.002404</td>\n",
       "      <td>0.001008</td>\n",
       "      <td>0.006454</td>\n",
       "      <td>0.002717</td>\n",
       "      <td>0.003125</td>\n",
       "      <td>0.003057</td>\n",
       "      <td>0.001488</td>\n",
       "      <td>0.003397</td>\n",
       "      <td>0.002734</td>\n",
       "      <td>0.003736</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>0.006641</td>\n",
       "      <td>0.002232</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.003057</td>\n",
       "      <td>0.004076</td>\n",
       "      <td>0.003057</td>\n",
       "      <td>0.003397</td>\n",
       "      <td>0.004076</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.984000</td>\n",
       "      <td>9.122238</td>\n",
       "      <td>7.676800</td>\n",
       "      <td>9.695738</td>\n",
       "      <td>8.762671</td>\n",
       "      <td>9.489078</td>\n",
       "      <td>9.751526</td>\n",
       "      <td>8.691076</td>\n",
       "      <td>9.013933</td>\n",
       "      <td>9.602705</td>\n",
       "      <td>...</td>\n",
       "      <td>9.673318</td>\n",
       "      <td>7.375434</td>\n",
       "      <td>9.672255</td>\n",
       "      <td>9.348755</td>\n",
       "      <td>9.299061</td>\n",
       "      <td>9.951019</td>\n",
       "      <td>9.036268</td>\n",
       "      <td>9.963328</td>\n",
       "      <td>9.505755</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 501 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              dim1         dim2         dim3         dim4         dim5  \\\n",
       "count  2113.000000  2113.000000  2113.000000  2113.000000  2113.000000   \n",
       "mean      0.043353     0.050830     0.047988     0.037411     0.043365   \n",
       "std       0.513260     0.582839     0.482166     0.464611     0.491187   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000868     0.000000     0.001359     0.000781     0.001116   \n",
       "50%       0.001616     0.000340     0.003516     0.001698     0.002038   \n",
       "75%       0.002404     0.001008     0.006454     0.002717     0.003125   \n",
       "max       9.984000     9.122238     7.676800     9.695738     8.762671   \n",
       "\n",
       "              dim6         dim7         dim8         dim9        dim10  \\\n",
       "count  2113.000000  2113.000000  2113.000000  2113.000000  2113.000000   \n",
       "mean      0.049670     0.051071     0.043601     0.052698     0.054139   \n",
       "std       0.558422     0.577599     0.472706     0.571629     0.588484   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.001019     0.000340     0.000651     0.000756     0.001116   \n",
       "50%       0.001860     0.000756     0.001645     0.001698     0.002155   \n",
       "75%       0.003057     0.001488     0.003397     0.002734     0.003736   \n",
       "max       9.489078     9.751526     8.691076     9.013933     9.602705   \n",
       "\n",
       "          ...            dim492       dim493       dim494       dim495  \\\n",
       "count     ...       2113.000000  2113.000000  2113.000000  2113.000000   \n",
       "mean      ...          0.052482     0.044370     0.046100     0.043218   \n",
       "std       ...          0.616955     0.466802     0.528117     0.508790   \n",
       "min       ...          0.000000     0.000000     0.000000     0.000000   \n",
       "25%       ...          0.000000     0.000679     0.000679     0.001764   \n",
       "50%       ...          0.000000     0.002717     0.001359     0.003125   \n",
       "75%       ...          0.000679     0.006641     0.002232     0.004883   \n",
       "max       ...          9.673318     7.375434     9.672255     9.348755   \n",
       "\n",
       "            dim496       dim497       dim498       dim499       dim500  \\\n",
       "count  2113.000000  2113.000000  2113.000000  2113.000000  2113.000000   \n",
       "mean      0.049724     0.050818     0.038214     0.050150     0.051801   \n",
       "std       0.561206     0.562799     0.437885     0.581028     0.569857   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.001019     0.000744     0.001116     0.001019     0.001019   \n",
       "50%       0.001953     0.002038     0.002038     0.002038     0.002268   \n",
       "75%       0.003057     0.004076     0.003057     0.003397     0.004076   \n",
       "max       9.299061     9.951019     9.036268     9.963328     9.505755   \n",
       "\n",
       "         is_person  \n",
       "count  2113.000000  \n",
       "mean      0.448178  \n",
       "std       0.497425  \n",
       "min       0.000000  \n",
       "25%       0.000000  \n",
       "50%       0.000000  \n",
       "75%       1.000000  \n",
       "max       1.000000  \n",
       "\n",
       "[8 rows x 501 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imTrain.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dim1</th>\n",
       "      <th>dim2</th>\n",
       "      <th>dim3</th>\n",
       "      <th>dim4</th>\n",
       "      <th>dim5</th>\n",
       "      <th>dim6</th>\n",
       "      <th>dim7</th>\n",
       "      <th>dim8</th>\n",
       "      <th>dim9</th>\n",
       "      <th>dim10</th>\n",
       "      <th>...</th>\n",
       "      <th>dim492</th>\n",
       "      <th>dim493</th>\n",
       "      <th>dim494</th>\n",
       "      <th>dim495</th>\n",
       "      <th>dim496</th>\n",
       "      <th>dim497</th>\n",
       "      <th>dim498</th>\n",
       "      <th>dim499</th>\n",
       "      <th>dim500</th>\n",
       "      <th>is_person</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1113.000000</td>\n",
       "      <td>1113.000000</td>\n",
       "      <td>1113.000000</td>\n",
       "      <td>1113.000000</td>\n",
       "      <td>1113.000000</td>\n",
       "      <td>1113.000000</td>\n",
       "      <td>1113.000000</td>\n",
       "      <td>1113.000000</td>\n",
       "      <td>1113.000000</td>\n",
       "      <td>1113.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1113.000000</td>\n",
       "      <td>1113.000000</td>\n",
       "      <td>1113.000000</td>\n",
       "      <td>1113.000000</td>\n",
       "      <td>1113.000000</td>\n",
       "      <td>1113.000000</td>\n",
       "      <td>1113.000000</td>\n",
       "      <td>1113.000000</td>\n",
       "      <td>1113.000000</td>\n",
       "      <td>1113.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.001744</td>\n",
       "      <td>0.000702</td>\n",
       "      <td>0.004720</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.002321</td>\n",
       "      <td>0.002110</td>\n",
       "      <td>0.001037</td>\n",
       "      <td>0.002529</td>\n",
       "      <td>0.002006</td>\n",
       "      <td>0.002641</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000598</td>\n",
       "      <td>0.004817</td>\n",
       "      <td>0.001585</td>\n",
       "      <td>0.003611</td>\n",
       "      <td>0.002196</td>\n",
       "      <td>0.002772</td>\n",
       "      <td>0.002239</td>\n",
       "      <td>0.002407</td>\n",
       "      <td>0.003097</td>\n",
       "      <td>0.473495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.001209</td>\n",
       "      <td>0.001364</td>\n",
       "      <td>0.003876</td>\n",
       "      <td>0.001417</td>\n",
       "      <td>0.001558</td>\n",
       "      <td>0.001559</td>\n",
       "      <td>0.000885</td>\n",
       "      <td>0.002736</td>\n",
       "      <td>0.001919</td>\n",
       "      <td>0.002293</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001241</td>\n",
       "      <td>0.005831</td>\n",
       "      <td>0.001243</td>\n",
       "      <td>0.002471</td>\n",
       "      <td>0.001575</td>\n",
       "      <td>0.003182</td>\n",
       "      <td>0.001346</td>\n",
       "      <td>0.001868</td>\n",
       "      <td>0.002590</td>\n",
       "      <td>0.499521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000744</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001698</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.001172</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.000340</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>0.000781</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>0.001860</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>0.001250</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.001359</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.001563</td>\n",
       "      <td>0.000340</td>\n",
       "      <td>0.003736</td>\n",
       "      <td>0.001698</td>\n",
       "      <td>0.002038</td>\n",
       "      <td>0.001803</td>\n",
       "      <td>0.000781</td>\n",
       "      <td>0.001698</td>\n",
       "      <td>0.001563</td>\n",
       "      <td>0.002038</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002734</td>\n",
       "      <td>0.001359</td>\n",
       "      <td>0.003125</td>\n",
       "      <td>0.001860</td>\n",
       "      <td>0.001838</td>\n",
       "      <td>0.002038</td>\n",
       "      <td>0.002038</td>\n",
       "      <td>0.002604</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.002378</td>\n",
       "      <td>0.000758</td>\n",
       "      <td>0.007102</td>\n",
       "      <td>0.002734</td>\n",
       "      <td>0.003057</td>\n",
       "      <td>0.002976</td>\n",
       "      <td>0.001488</td>\n",
       "      <td>0.003397</td>\n",
       "      <td>0.002717</td>\n",
       "      <td>0.003397</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>0.006793</td>\n",
       "      <td>0.002232</td>\n",
       "      <td>0.004836</td>\n",
       "      <td>0.003057</td>\n",
       "      <td>0.003736</td>\n",
       "      <td>0.003057</td>\n",
       "      <td>0.003348</td>\n",
       "      <td>0.004092</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.007133</td>\n",
       "      <td>0.022135</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.008929</td>\n",
       "      <td>0.010417</td>\n",
       "      <td>0.010789</td>\n",
       "      <td>0.005757</td>\n",
       "      <td>0.022396</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.013927</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012747</td>\n",
       "      <td>0.042026</td>\n",
       "      <td>0.009821</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.013346</td>\n",
       "      <td>0.029225</td>\n",
       "      <td>0.007068</td>\n",
       "      <td>0.011889</td>\n",
       "      <td>0.026786</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 501 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              dim1         dim2         dim3         dim4         dim5  \\\n",
       "count  1113.000000  1113.000000  1113.000000  1113.000000  1113.000000   \n",
       "mean      0.001744     0.000702     0.004720     0.001978     0.002321   \n",
       "std       0.001209     0.001364     0.003876     0.001417     0.001558   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000744     0.000000     0.001698     0.001019     0.001172   \n",
       "50%       0.001563     0.000340     0.003736     0.001698     0.002038   \n",
       "75%       0.002378     0.000758     0.007102     0.002734     0.003057   \n",
       "max       0.007133     0.022135     0.023438     0.008929     0.010417   \n",
       "\n",
       "              dim6         dim7         dim8         dim9        dim10  \\\n",
       "count  1113.000000  1113.000000  1113.000000  1113.000000  1113.000000   \n",
       "mean      0.002110     0.001037     0.002529     0.002006     0.002641   \n",
       "std       0.001559     0.000885     0.002736     0.001919     0.002293   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.001019     0.000340     0.000679     0.000781     0.001019   \n",
       "50%       0.001803     0.000781     0.001698     0.001563     0.002038   \n",
       "75%       0.002976     0.001488     0.003397     0.002717     0.003397   \n",
       "max       0.010789     0.005757     0.022396     0.025000     0.013927   \n",
       "\n",
       "          ...            dim492       dim493       dim494       dim495  \\\n",
       "count     ...       1113.000000  1113.000000  1113.000000  1113.000000   \n",
       "mean      ...          0.000598     0.004817     0.001585     0.003611   \n",
       "std       ...          0.001241     0.005831     0.001243     0.002471   \n",
       "min       ...          0.000000     0.000000     0.000000     0.000000   \n",
       "25%       ...          0.000000     0.000679     0.000679     0.001860   \n",
       "50%       ...          0.000000     0.002734     0.001359     0.003125   \n",
       "75%       ...          0.000679     0.006793     0.002232     0.004836   \n",
       "max       ...          0.012747     0.042026     0.009821     0.015625   \n",
       "\n",
       "            dim496       dim497       dim498       dim499       dim500  \\\n",
       "count  1113.000000  1113.000000  1113.000000  1113.000000  1113.000000   \n",
       "mean      0.002196     0.002772     0.002239     0.002407     0.003097   \n",
       "std       0.001575     0.003182     0.001346     0.001868     0.002590   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.001019     0.000679     0.001250     0.001019     0.001359   \n",
       "50%       0.001860     0.001838     0.002038     0.002038     0.002604   \n",
       "75%       0.003057     0.003736     0.003057     0.003348     0.004092   \n",
       "max       0.013346     0.029225     0.007068     0.011889     0.026786   \n",
       "\n",
       "         is_person  \n",
       "count  1113.000000  \n",
       "mean      0.473495  \n",
       "std       0.499521  \n",
       "min       0.000000  \n",
       "25%       0.000000  \n",
       "50%       0.000000  \n",
       "75%       1.000000  \n",
       "max       1.000000  \n",
       "\n",
       "[8 rows x 501 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imVal.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dim1</th>\n",
       "      <th>dim2</th>\n",
       "      <th>dim3</th>\n",
       "      <th>dim4</th>\n",
       "      <th>dim5</th>\n",
       "      <th>dim6</th>\n",
       "      <th>dim7</th>\n",
       "      <th>dim8</th>\n",
       "      <th>dim9</th>\n",
       "      <th>dim10</th>\n",
       "      <th>...</th>\n",
       "      <th>dim491</th>\n",
       "      <th>dim492</th>\n",
       "      <th>dim493</th>\n",
       "      <th>dim494</th>\n",
       "      <th>dim495</th>\n",
       "      <th>dim496</th>\n",
       "      <th>dim497</th>\n",
       "      <th>dim498</th>\n",
       "      <th>dim499</th>\n",
       "      <th>dim500</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1114.000000</td>\n",
       "      <td>1114.000000</td>\n",
       "      <td>1114.000000</td>\n",
       "      <td>1114.000000</td>\n",
       "      <td>1114.000000</td>\n",
       "      <td>1114.000000</td>\n",
       "      <td>1114.000000</td>\n",
       "      <td>1114.000000</td>\n",
       "      <td>1114.000000</td>\n",
       "      <td>1114.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1114.000000</td>\n",
       "      <td>1114.000000</td>\n",
       "      <td>1114.000000</td>\n",
       "      <td>1114.000000</td>\n",
       "      <td>1114.000000</td>\n",
       "      <td>1114.000000</td>\n",
       "      <td>1114.000000</td>\n",
       "      <td>1114.000000</td>\n",
       "      <td>1114.000000</td>\n",
       "      <td>1114.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.001754</td>\n",
       "      <td>0.000717</td>\n",
       "      <td>0.004379</td>\n",
       "      <td>0.001839</td>\n",
       "      <td>0.002207</td>\n",
       "      <td>0.002105</td>\n",
       "      <td>0.001058</td>\n",
       "      <td>0.002535</td>\n",
       "      <td>0.002053</td>\n",
       "      <td>0.002746</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002033</td>\n",
       "      <td>0.000577</td>\n",
       "      <td>0.004880</td>\n",
       "      <td>0.001537</td>\n",
       "      <td>0.003591</td>\n",
       "      <td>0.002080</td>\n",
       "      <td>0.002897</td>\n",
       "      <td>0.002216</td>\n",
       "      <td>0.002438</td>\n",
       "      <td>0.003017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.001226</td>\n",
       "      <td>0.001522</td>\n",
       "      <td>0.003648</td>\n",
       "      <td>0.001320</td>\n",
       "      <td>0.001539</td>\n",
       "      <td>0.001612</td>\n",
       "      <td>0.000863</td>\n",
       "      <td>0.002867</td>\n",
       "      <td>0.001990</td>\n",
       "      <td>0.002470</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001597</td>\n",
       "      <td>0.001146</td>\n",
       "      <td>0.006427</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>0.002626</td>\n",
       "      <td>0.001519</td>\n",
       "      <td>0.003240</td>\n",
       "      <td>0.001449</td>\n",
       "      <td>0.001897</td>\n",
       "      <td>0.002795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001488</td>\n",
       "      <td>0.000744</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.000340</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>0.000744</td>\n",
       "      <td>0.001065</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000791</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>0.001698</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.000744</td>\n",
       "      <td>0.001065</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.001019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.001590</td>\n",
       "      <td>0.000340</td>\n",
       "      <td>0.003397</td>\n",
       "      <td>0.001698</td>\n",
       "      <td>0.001860</td>\n",
       "      <td>0.001776</td>\n",
       "      <td>0.000822</td>\n",
       "      <td>0.001596</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>0.002115</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001698</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002717</td>\n",
       "      <td>0.001359</td>\n",
       "      <td>0.003057</td>\n",
       "      <td>0.001813</td>\n",
       "      <td>0.002038</td>\n",
       "      <td>0.002038</td>\n",
       "      <td>0.002038</td>\n",
       "      <td>0.002361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.002378</td>\n",
       "      <td>0.000781</td>\n",
       "      <td>0.006454</td>\n",
       "      <td>0.002717</td>\n",
       "      <td>0.003057</td>\n",
       "      <td>0.002717</td>\n",
       "      <td>0.001698</td>\n",
       "      <td>0.003397</td>\n",
       "      <td>0.002717</td>\n",
       "      <td>0.003720</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002717</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>0.006793</td>\n",
       "      <td>0.002232</td>\n",
       "      <td>0.004755</td>\n",
       "      <td>0.002734</td>\n",
       "      <td>0.003831</td>\n",
       "      <td>0.003057</td>\n",
       "      <td>0.003463</td>\n",
       "      <td>0.004076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.009046</td>\n",
       "      <td>0.027699</td>\n",
       "      <td>0.021399</td>\n",
       "      <td>0.007473</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.011889</td>\n",
       "      <td>0.004755</td>\n",
       "      <td>0.021399</td>\n",
       "      <td>0.021140</td>\n",
       "      <td>0.022500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015253</td>\n",
       "      <td>0.010691</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.007576</td>\n",
       "      <td>0.026834</td>\n",
       "      <td>0.009851</td>\n",
       "      <td>0.029212</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.012228</td>\n",
       "      <td>0.028193</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              dim1         dim2         dim3         dim4         dim5  \\\n",
       "count  1114.000000  1114.000000  1114.000000  1114.000000  1114.000000   \n",
       "mean      0.001754     0.000717     0.004379     0.001839     0.002207   \n",
       "std       0.001226     0.001522     0.003648     0.001320     0.001539   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000977     0.000000     0.001488     0.000744     0.001019   \n",
       "50%       0.001590     0.000340     0.003397     0.001698     0.001860   \n",
       "75%       0.002378     0.000781     0.006454     0.002717     0.003057   \n",
       "max       0.009046     0.027699     0.021399     0.007473     0.009766   \n",
       "\n",
       "              dim6         dim7         dim8         dim9        dim10  \\\n",
       "count  1114.000000  1114.000000  1114.000000  1114.000000  1114.000000   \n",
       "mean      0.002105     0.001058     0.002535     0.002053     0.002746   \n",
       "std       0.001612     0.000863     0.002867     0.001990     0.002470   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.001019     0.000340     0.000679     0.000744     0.001065   \n",
       "50%       0.001776     0.000822     0.001596     0.001667     0.002115   \n",
       "75%       0.002717     0.001698     0.003397     0.002717     0.003720   \n",
       "max       0.011889     0.004755     0.021399     0.021140     0.022500   \n",
       "\n",
       "          ...            dim491       dim492       dim493       dim494  \\\n",
       "count     ...       1114.000000  1114.000000  1114.000000  1114.000000   \n",
       "mean      ...          0.002033     0.000577     0.004880     0.001537   \n",
       "std       ...          0.001597     0.001146     0.006427     0.001200   \n",
       "min       ...          0.000000     0.000000     0.000000     0.000000   \n",
       "25%       ...          0.000791     0.000000     0.000679     0.000679   \n",
       "50%       ...          0.001698     0.000000     0.002717     0.001359   \n",
       "75%       ...          0.002717     0.000679     0.006793     0.002232   \n",
       "max       ...          0.015253     0.010691     0.062500     0.007576   \n",
       "\n",
       "            dim495       dim496       dim497       dim498       dim499  \\\n",
       "count  1114.000000  1114.000000  1114.000000  1114.000000  1114.000000   \n",
       "mean      0.003591     0.002080     0.002897     0.002216     0.002438   \n",
       "std       0.002626     0.001519     0.003240     0.001449     0.001897   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.001698     0.001019     0.000744     0.001065     0.001019   \n",
       "50%       0.003057     0.001813     0.002038     0.002038     0.002038   \n",
       "75%       0.004755     0.002734     0.003831     0.003057     0.003463   \n",
       "max       0.026834     0.009851     0.029212     0.007812     0.012228   \n",
       "\n",
       "            dim500  \n",
       "count  1114.000000  \n",
       "mean      0.003017  \n",
       "std       0.002795  \n",
       "min       0.000000  \n",
       "25%       0.001019  \n",
       "50%       0.002361  \n",
       "75%       0.004076  \n",
       "max       0.028193  \n",
       "\n",
       "[8 rows x 500 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imTest.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see the training data has some outliers, so we will need to remove those first; applying anything we do to the training data to the other sets too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA68AAAIMCAYAAAAaZ5odAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1152x648 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(imTrain['dim1']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA68AAAIMCAYAAAAaZ5odAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<Figure size 1152x648 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(imTrain.query('dim1 < 1')['dim1']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA6gAAAIMCAYAAAD4u4FkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1152x648 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(imTrain.query('dim1 < 0.5')['dim1']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cutting off values less than 0.5 we can see that actually values don't exceed 0.01 apart from the few we can see (faintly) in the earlier plots so this should be a fair cutoff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dim1</th>\n",
       "      <th>dim2</th>\n",
       "      <th>dim3</th>\n",
       "      <th>dim4</th>\n",
       "      <th>dim5</th>\n",
       "      <th>dim6</th>\n",
       "      <th>dim7</th>\n",
       "      <th>dim8</th>\n",
       "      <th>dim9</th>\n",
       "      <th>dim10</th>\n",
       "      <th>...</th>\n",
       "      <th>dim492</th>\n",
       "      <th>dim493</th>\n",
       "      <th>dim494</th>\n",
       "      <th>dim495</th>\n",
       "      <th>dim496</th>\n",
       "      <th>dim497</th>\n",
       "      <th>dim498</th>\n",
       "      <th>dim499</th>\n",
       "      <th>dim500</th>\n",
       "      <th>is_person</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2093.000000</td>\n",
       "      <td>2093.000000</td>\n",
       "      <td>2093.000000</td>\n",
       "      <td>2093.000000</td>\n",
       "      <td>2093.000000</td>\n",
       "      <td>2093.000000</td>\n",
       "      <td>2093.000000</td>\n",
       "      <td>2093.000000</td>\n",
       "      <td>2093.000000</td>\n",
       "      <td>2093.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2093.000000</td>\n",
       "      <td>2093.000000</td>\n",
       "      <td>2093.000000</td>\n",
       "      <td>2093.000000</td>\n",
       "      <td>2093.000000</td>\n",
       "      <td>2093.000000</td>\n",
       "      <td>2093.000000</td>\n",
       "      <td>2093.000000</td>\n",
       "      <td>2093.000000</td>\n",
       "      <td>2093.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.001751</td>\n",
       "      <td>0.000756</td>\n",
       "      <td>0.004317</td>\n",
       "      <td>0.001853</td>\n",
       "      <td>0.002272</td>\n",
       "      <td>0.002220</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.002446</td>\n",
       "      <td>0.002088</td>\n",
       "      <td>0.002746</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000563</td>\n",
       "      <td>0.004586</td>\n",
       "      <td>0.001541</td>\n",
       "      <td>0.003642</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>0.002928</td>\n",
       "      <td>0.002173</td>\n",
       "      <td>0.002485</td>\n",
       "      <td>0.002978</td>\n",
       "      <td>0.449116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.001193</td>\n",
       "      <td>0.001406</td>\n",
       "      <td>0.003693</td>\n",
       "      <td>0.001340</td>\n",
       "      <td>0.001598</td>\n",
       "      <td>0.001741</td>\n",
       "      <td>0.000873</td>\n",
       "      <td>0.002910</td>\n",
       "      <td>0.001971</td>\n",
       "      <td>0.002328</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001203</td>\n",
       "      <td>0.005825</td>\n",
       "      <td>0.001192</td>\n",
       "      <td>0.002657</td>\n",
       "      <td>0.001664</td>\n",
       "      <td>0.003254</td>\n",
       "      <td>0.001418</td>\n",
       "      <td>0.001997</td>\n",
       "      <td>0.002765</td>\n",
       "      <td>0.497523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000833</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001359</td>\n",
       "      <td>0.000756</td>\n",
       "      <td>0.001116</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.000340</td>\n",
       "      <td>0.000625</td>\n",
       "      <td>0.000744</td>\n",
       "      <td>0.001116</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>0.001698</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.000744</td>\n",
       "      <td>0.001116</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.001563</td>\n",
       "      <td>0.000340</td>\n",
       "      <td>0.003397</td>\n",
       "      <td>0.001698</td>\n",
       "      <td>0.002038</td>\n",
       "      <td>0.001860</td>\n",
       "      <td>0.000744</td>\n",
       "      <td>0.001563</td>\n",
       "      <td>0.001698</td>\n",
       "      <td>0.002056</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002717</td>\n",
       "      <td>0.001359</td>\n",
       "      <td>0.003057</td>\n",
       "      <td>0.001860</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.002038</td>\n",
       "      <td>0.002038</td>\n",
       "      <td>0.002232</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.002378</td>\n",
       "      <td>0.000893</td>\n",
       "      <td>0.006324</td>\n",
       "      <td>0.002717</td>\n",
       "      <td>0.003057</td>\n",
       "      <td>0.003057</td>\n",
       "      <td>0.001488</td>\n",
       "      <td>0.003397</td>\n",
       "      <td>0.002717</td>\n",
       "      <td>0.003736</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>0.006454</td>\n",
       "      <td>0.002232</td>\n",
       "      <td>0.004808</td>\n",
       "      <td>0.003057</td>\n",
       "      <td>0.003780</td>\n",
       "      <td>0.002976</td>\n",
       "      <td>0.003397</td>\n",
       "      <td>0.004076</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.009851</td>\n",
       "      <td>0.016644</td>\n",
       "      <td>0.027514</td>\n",
       "      <td>0.010789</td>\n",
       "      <td>0.010417</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.005774</td>\n",
       "      <td>0.029830</td>\n",
       "      <td>0.028372</td>\n",
       "      <td>0.020380</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.053329</td>\n",
       "      <td>0.010234</td>\n",
       "      <td>0.024457</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.028125</td>\n",
       "      <td>0.008492</td>\n",
       "      <td>0.014509</td>\n",
       "      <td>0.028533</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 501 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              dim1         dim2         dim3         dim4         dim5  \\\n",
       "count  2093.000000  2093.000000  2093.000000  2093.000000  2093.000000   \n",
       "mean      0.001751     0.000756     0.004317     0.001853     0.002272   \n",
       "std       0.001193     0.001406     0.003693     0.001340     0.001598   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000833     0.000000     0.001359     0.000756     0.001116   \n",
       "50%       0.001563     0.000340     0.003397     0.001698     0.002038   \n",
       "75%       0.002378     0.000893     0.006324     0.002717     0.003057   \n",
       "max       0.009851     0.016644     0.027514     0.010789     0.010417   \n",
       "\n",
       "              dim6         dim7         dim8         dim9        dim10  \\\n",
       "count  2093.000000  2093.000000  2093.000000  2093.000000  2093.000000   \n",
       "mean      0.002220     0.001019     0.002446     0.002088     0.002746   \n",
       "std       0.001741     0.000873     0.002910     0.001971     0.002328   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.001019     0.000340     0.000625     0.000744     0.001116   \n",
       "50%       0.001860     0.000744     0.001563     0.001698     0.002056   \n",
       "75%       0.003057     0.001488     0.003397     0.002717     0.003736   \n",
       "max       0.021739     0.005774     0.029830     0.028372     0.020380   \n",
       "\n",
       "          ...            dim492       dim493       dim494       dim495  \\\n",
       "count     ...       2093.000000  2093.000000  2093.000000  2093.000000   \n",
       "mean      ...          0.000563     0.004586     0.001541     0.003642   \n",
       "std       ...          0.001203     0.005825     0.001192     0.002657   \n",
       "min       ...          0.000000     0.000000     0.000000     0.000000   \n",
       "25%       ...          0.000000     0.000679     0.000679     0.001698   \n",
       "50%       ...          0.000000     0.002717     0.001359     0.003057   \n",
       "75%       ...          0.000679     0.006454     0.002232     0.004808   \n",
       "max       ...          0.021739     0.053329     0.010234     0.024457   \n",
       "\n",
       "            dim496       dim497       dim498       dim499       dim500  \\\n",
       "count  2093.000000  2093.000000  2093.000000  2093.000000  2093.000000   \n",
       "mean      0.002200     0.002928     0.002173     0.002485     0.002978   \n",
       "std       0.001664     0.003254     0.001418     0.001997     0.002765   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.001019     0.000744     0.001116     0.001019     0.001019   \n",
       "50%       0.001860     0.001953     0.002038     0.002038     0.002232   \n",
       "75%       0.003057     0.003780     0.002976     0.003397     0.004076   \n",
       "max       0.011719     0.028125     0.008492     0.014509     0.028533   \n",
       "\n",
       "         is_person  \n",
       "count  2093.000000  \n",
       "mean      0.449116  \n",
       "std       0.497523  \n",
       "min       0.000000  \n",
       "25%       0.000000  \n",
       "50%       0.000000  \n",
       "75%       1.000000  \n",
       "max       1.000000  \n",
       "\n",
       "[8 rows x 501 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imTrain = imTrain.query('dim1 < 0.1')\n",
    "imTrain.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The data now looks more reasonable, let's do this to the rest of the data so there is no bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imTest = imTest.query('dim1 < 0.1')\n",
    "imVal  = imVal.query('dim1 < 0.1')\n",
    "\n",
    "X_train = imTrain.drop('is_person', axis=1)\n",
    "y_train = imTrain['is_person']\n",
    "X_test  = imTest.drop('is_person', axis=1)\n",
    "y_test  = imTest['is_person']\n",
    "X_val   = imVal.drop('is_person', axis=1)\n",
    "y_val   = imVal['is_person']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Scaling the data should help matters, and we can do this in many ways. I will try StandardScaler and MinMaxScaler.\n",
    "- PCA might also help, especially with some classifiers\n",
    "- There are many classifiers we can try. I will attempt Gaussian Naive Bayes, Random Forrest, Logistics Regression and Support Vector Machines\n",
    "- Within each of these there are some parameters that I will try to optimise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\ipykernel_launcher.py:3: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "standScaler    = StandardScaler().fit(X_train)\n",
    "sscaledX_train = standScaler.transform(X_train)\n",
    "sscaledX_val   = standScaler.transform(X_val)\n",
    "sscaledX_test  = standScaler.transform(X_test)\n",
    "\n",
    "minmaxScaler    = MinMaxScaler().fit(X_train)\n",
    "mmscaledX_train = minmaxScaler.transform(X_train)\n",
    "mmscaledX_val   = minmaxScaler.transform(X_val)\n",
    "mmscaledX_test  = minmaxScaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6442048517520216"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ssGNB = GaussianNB()\n",
    "ssGNB.fit(sscaledX_train, y_train)\n",
    "\n",
    "ssGNB.score(sscaledX_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6442048517520216"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mmGNB = GaussianNB()\n",
    "mmGNB.fit(mmscaledX_train, y_train)\n",
    "mmGNB.score(mmscaledX_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| n =     1 | score = 0.569 |\n",
      "| n =     1 | score = 0.569 |\n",
      "| n =     1 | score = 0.569 |\n",
      "| n =     2 | score = 0.623 |\n",
      "| n =     3 | score = 0.634 |\n",
      "| n =     5 | score = 0.647 |\n",
      "| n =     7 | score = 0.668 |\n",
      "| n =     9 | score = 0.662 |\n",
      "| n =    13 | score = 0.668 |\n",
      "| n =    18 | score = 0.668 |\n",
      "| n =    26 | score = 0.663 |\n",
      "| n =    36 | score = 0.649 |\n",
      "| n =    50 | score = 0.645 |\n",
      "| n =    70 | score = 0.645 |\n",
      "| n =    97 | score = 0.646 |\n",
      "| n =   135 | score = 0.642 |\n",
      "| n =   187 | score = 0.656 |\n",
      "| n =   259 | score = 0.675 |\n",
      "| n =   360 | score = 0.684 |\n",
      "| n =   500 | score = 0.700 |\n"
     ]
    }
   ],
   "source": [
    "n_vals = np.logspace(0, 2.699, 20)\n",
    "for n in n_vals:\n",
    "    PCAn = PCA(int(n), random_state=1000)\n",
    "    GNBn = GaussianNB()\n",
    "    X_new = PCAn.fit_transform(sscaledX_train)\n",
    "    GNBn.fit(X_new, y_train)\n",
    "    print(\"| n = {0:>5} | score = {1:.3f} |\".format(int(n), GNBn.score(X_new, y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6639712488769093"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PCA500 = PCA(500, random_state=1000)\n",
    "GNB500 = GaussianNB()\n",
    "PCA500 = PCA500.fit(sscaledX_train)\n",
    "X_train_new = PCA500.transform(sscaledX_train)\n",
    "X_val_new = PCA500.transform(sscaledX_val)\n",
    "GNB500.fit(X_train_new, y_train)\n",
    "GNB500.score(X_val_new, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4688492543211131"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnbProbs = GNB500.predict_proba(X_val_new)\n",
    "log_loss(y_val, gnbProbs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Doing the PCA on Gaussian Naive Bayes with n=500 (the total number of features) improves accuracy which makes sense since the PCA tries to produce features that are perpendicular in the n-dimensional space and this is in line with the assumption of Naive Bayes that each feature is conditionally independent.\n",
    "- It is interesting here to see the classification accuracy of the Naive approach is fairly good, but the log loss shows us that the predictions are not very stable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forrest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score on training: 1.0\n",
      "Score on validation: 0.706199460916442\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=100, criterion='entropy')\n",
    "rf.fit(sscaledX_train, y_train)\n",
    "print(\"Score on training:\", rf.score(sscaledX_train, y_train))\n",
    "print(\"Score on validation:\", rf.score(sscaledX_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Perhaps the classifier is over-fitting to the training data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| max depth:   1 | train: 0.658 | val: 0.625 |\n",
      "| max depth:   2 | train: 0.699 | val: 0.660 |\n",
      "| max depth:   3 | train: 0.723 | val: 0.668 |\n",
      "| max depth:   4 | train: 0.757 | val: 0.672 |\n",
      "| max depth:   5 | train: 0.815 | val: 0.670 |\n",
      "| max depth:   6 | train: 0.881 | val: 0.677 |\n",
      "| max depth:   7 | train: 0.938 | val: 0.672 |\n",
      "| max depth:   8 | train: 0.977 | val: 0.686 |\n",
      "| max depth:   9 | train: 0.996 | val: 0.689 |\n",
      "| max depth:  10 | train: 1.000 | val: 0.694 |\n",
      "| max depth:  11 | train: 1.000 | val: 0.686 |\n",
      "| max depth:  12 | train: 1.000 | val: 0.687 |\n",
      "| max depth:  13 | train: 1.000 | val: 0.690 |\n",
      "| max depth:  14 | train: 1.000 | val: 0.696 |\n",
      "| max depth:  15 | train: 1.000 | val: 0.695 |\n",
      "| max depth:  16 | train: 1.000 | val: 0.688 |\n",
      "| max depth:  17 | train: 1.000 | val: 0.708 |\n",
      "| max depth:  18 | train: 1.000 | val: 0.709 |\n",
      "| max depth:  19 | train: 1.000 | val: 0.694 |\n"
     ]
    }
   ],
   "source": [
    "depths = list(range(1, 20))\n",
    "for d in depths:\n",
    "    rfd = RandomForestClassifier(n_estimators=100, criterion='entropy', max_depth=d)\n",
    "    rfd.fit(sscaledX_train, y_train)\n",
    "    print(\"| max depth: {0:>3} | train: {1:.3f} | val: {2:.3f} |\".format(d, rfd.score(sscaledX_train, y_train), rfd.score(sscaledX_val, y_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Actually, we only see the best validation set scores when the accuracy on the training data is 100% so this is fine, but it was good to check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best value of n was 57 getting a score on the validation set of 0.7124887690925427.\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "n_vals = np.logspace(0, 2.699, 50)\n",
    "n_vals = list(sorted(set([ int(n) for n in n_vals ])))\n",
    "for n in n_vals:\n",
    "    PCAn = PCA(int(n), random_state=1000).fit(sscaledX_train)\n",
    "    rfn = RandomForestClassifier(n_estimators=100, criterion='entropy', max_depth=18)\n",
    "    X_new = PCAn.transform(sscaledX_train)\n",
    "    X_val_new = PCAn.transform(sscaledX_val)\n",
    "    rfn.fit(X_new, y_train)\n",
    "    scores.append(rfn.score(X_val_new, y_val))\n",
    "    #print(\"| n = {0:>4} | score = {1:.3f} |\".format(int(n), rfn.score(X_val_new, y_val)))\n",
    "    \n",
    "best = np.argmax(scores)\n",
    "print(\"Best value of n was {0} getting a score on the validation set of {1}.\".format(int(n_vals[best]), scores[best]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5797040362577033"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PCAn = PCA(57, random_state=1000).fit(sscaledX_train)\n",
    "rfn = RandomForestClassifier(n_estimators=100, criterion='entropy', max_depth=18)\n",
    "X_new = PCAn.transform(sscaledX_train)\n",
    "X_val_new = PCAn.transform(sscaledX_val)\n",
    "rfn.fit(X_new, y_train)\n",
    "\n",
    "forPred = rfn.predict_proba(X_val_new)\n",
    "log_loss(y_val, forPred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Random Forest does better, but adds a lot of complexity on Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best value of n was 16 getting a score on the validation set of 0.615.\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "for n in n_vals:\n",
    "    logRK = LogisticRegression(solver='lbfgs')\n",
    "    PCAn = PCA(int(n), random_state=1000).fit(sscaledX_train[:-200])\n",
    "    X_new = PCAn.transform(sscaledX_train[:-200])\n",
    "    logRK.fit(X_new, y_train[:-200])\n",
    "    X_val_new = PCAn.transform(sscaledX_val[-200:])\n",
    "    scores.append(logRK.score(X_val_new, y_train[-200:]))\n",
    "    # Un-comment this line to get middling info\n",
    "    #print(\"| n = {0:>4} | score: {1:.3f} |\".format(int(n), logRK.score(X_val_new, y_train[-200:])))\n",
    "    \n",
    "best = np.argmax(scores)\n",
    "print(\"Best value of n was {0} getting a score on the validation set of {1:.3f}.\".format(int(n_vals[best]), scores[best]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I did the above to see what the best value of n was when using PCA on the data, but I found that this actually varied depending on the C value of the classifier and the solver type so I did a more in-depth investigation below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. The Standard Scaled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using solver:  liblinear\n",
      "\t| n =    1 | C = 4.833e-04 | score: 0.558 |\n",
      "\t| n =    2 | C = 4.833e-04 | score: 0.621 |\n",
      "\t| n =    3 | C = 2.336e-02 | score: 0.629 |\n",
      "\t| n =    4 | C = 3.360e-03 | score: 0.638 |\n",
      "\t| n =    5 | C = 8.859e-03 | score: 0.652 |\n",
      "\t| n =    6 | C = 1.129e+00 | score: 0.658 |\n",
      "\t| n =    7 | C = 3.360e-03 | score: 0.667 |\n",
      "\t| n =    8 | C = 8.859e-03 | score: 0.668 |\n",
      "\t| n =    9 | C = 8.859e-03 | score: 0.668 |\n",
      "\t| n =   11 | C = 7.848e+00 | score: 0.668 |\n",
      "\t| n =   12 | C = 1.274e-03 | score: 0.667 |\n",
      "\t| n =   14 | C = 3.360e-03 | score: 0.670 |\n",
      "\t| n =   16 | C = 6.158e-02 | score: 0.680 |\n",
      "\t| n =   18 | C = 2.336e-02 | score: 0.689 |\n",
      "\t| n =   20 | C = 1.129e+00 | score: 0.687 |\n",
      "\t| n =   23 | C = 4.281e-01 | score: 0.688 |\n",
      "\t| n =   27 | C = 2.976e+00 | score: 0.689 |\n",
      "\t| n =   30 | C = 3.360e-03 | score: 0.686 |\n",
      "\t| n =   34 | C = 1.129e+00 | score: 0.684 |\n",
      "\t| n =   39 | C = 1.624e-01 | score: 0.683 |\n",
      "\t| n =   44 | C = 2.336e-02 | score: 0.692 |\n",
      "\t| n =   50 | C = 1.624e-01 | score: 0.689 |\n",
      "\t| n =   57 | C = 1.129e+00 | score: 0.694 |\n",
      "\t| n =   65 | C = 6.158e-02 | score: 0.685 |\n",
      "\t| n =   74 | C = 2.336e-02 | score: 0.688 |\n",
      "\t| n =   84 | C = 3.360e-03 | score: 0.688 |\n",
      "\t| n =   96 | C = 8.859e-03 | score: 0.691 |\n",
      "\t| n =  109 | C = 3.360e-03 | score: 0.688 |\n",
      "\t| n =  123 | C = 1.129e+00 | score: 0.688 |\n",
      "\t| n =  140 | C = 8.859e-03 | score: 0.688 |\n",
      "\t| n =  159 | C = 2.336e-02 | score: 0.690 |\n",
      "\t| n =  181 | C = 8.859e-03 | score: 0.690 |\n",
      "\t| n =  205 | C = 3.360e-03 | score: 0.690 |\n",
      "\t| n =  233 | C = 3.360e-03 | score: 0.697 |\n",
      "\t| n =  265 | C = 3.360e-03 | score: 0.696 |\n",
      "\t| n =  301 | C = 3.360e-03 | score: 0.695 |\n",
      "\t| n =  341 | C = 3.360e-03 | score: 0.691 |\n",
      "\t| n =  388 | C = 3.360e-03 | score: 0.693 |\n",
      "\t| n =  440 | C = 8.859e-03 | score: 0.698 |\n",
      "\t| n =  500 | C = 3.360e-03 | score: 0.692 |\n",
      "\tBest score was 0.698 with C = 8.86e-03 and n = 440\n",
      "Using solver:  lbfgs\n",
      "\t| n =    1 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| n =    2 | C = 2.336e-02 | score: 0.613 |\n",
      "\t| n =    3 | C = 1.833e-04 | score: 0.631 |\n",
      "\t| n =    4 | C = 1.833e-04 | score: 0.639 |\n",
      "\t| n =    5 | C = 8.859e-03 | score: 0.649 |\n",
      "\t| n =    6 | C = 3.360e-03 | score: 0.659 |\n",
      "\t| n =    7 | C = 3.360e-03 | score: 0.665 |\n",
      "\t| n =    8 | C = 3.360e-03 | score: 0.666 |\n",
      "\t| n =    9 | C = 4.281e-01 | score: 0.664 |\n",
      "\t| n =   11 | C = 8.859e-03 | score: 0.669 |\n",
      "\t| n =   12 | C = 8.859e-03 | score: 0.665 |\n",
      "\t| n =   14 | C = 8.859e-03 | score: 0.668 |\n",
      "\t| n =   16 | C = 4.281e-01 | score: 0.680 |\n",
      "\t| n =   18 | C = 6.158e-02 | score: 0.690 |\n",
      "\t| n =   20 | C = 3.360e-03 | score: 0.691 |\n",
      "\t| n =   23 | C = 3.360e-03 | score: 0.691 |\n",
      "\t| n =   27 | C = 1.274e-03 | score: 0.694 |\n",
      "\t| n =   30 | C = 3.360e-03 | score: 0.691 |\n",
      "\t| n =   34 | C = 1.274e-03 | score: 0.689 |\n",
      "\t| n =   39 | C = 1.274e-03 | score: 0.688 |\n",
      "\t| n =   44 | C = 8.859e-03 | score: 0.693 |\n",
      "\t| n =   50 | C = 8.859e-03 | score: 0.695 |\n",
      "\t| n =   57 | C = 6.158e-02 | score: 0.694 |\n",
      "\t| n =   65 | C = 3.360e-03 | score: 0.689 |\n",
      "\t| n =   74 | C = 2.336e-02 | score: 0.687 |\n",
      "\t| n =   84 | C = 2.336e-02 | score: 0.688 |\n",
      "\t| n =   96 | C = 3.360e-03 | score: 0.693 |\n",
      "\t| n =  109 | C = 8.859e-03 | score: 0.693 |\n",
      "\t| n =  123 | C = 3.360e-03 | score: 0.691 |\n",
      "\t| n =  140 | C = 3.360e-03 | score: 0.690 |\n",
      "\t| n =  159 | C = 1.274e-03 | score: 0.692 |\n",
      "\t| n =  181 | C = 3.360e-03 | score: 0.696 |\n",
      "\t| n =  205 | C = 3.360e-03 | score: 0.697 |\n",
      "\t| n =  233 | C = 3.360e-03 | score: 0.699 |\n",
      "\t| n =  265 | C = 3.360e-03 | score: 0.699 |\n",
      "\t| n =  301 | C = 3.360e-03 | score: 0.702 |\n",
      "\t| n =  341 | C = 3.360e-03 | score: 0.699 |\n",
      "\t| n =  388 | C = 3.360e-03 | score: 0.706 |\n",
      "\t| n =  440 | C = 3.360e-03 | score: 0.709 |\n",
      "\t| n =  500 | C = 3.360e-03 | score: 0.706 |\n",
      "\tBest score was 0.709 with C = 3.36e-03 and n = 440\n",
      "Using solver:  sag\n",
      "\t| n =    1 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| n =    2 | C = 2.336e-02 | score: 0.613 |\n",
      "\t| n =    3 | C = 1.833e-04 | score: 0.631 |\n",
      "\t| n =    4 | C = 1.833e-04 | score: 0.639 |\n",
      "\t| n =    5 | C = 8.859e-03 | score: 0.649 |\n",
      "\t| n =    6 | C = 3.360e-03 | score: 0.659 |\n",
      "\t| n =    7 | C = 3.360e-03 | score: 0.665 |\n",
      "\t| n =    8 | C = 3.360e-03 | score: 0.666 |\n",
      "\t| n =    9 | C = 1.129e+00 | score: 0.664 |\n",
      "\t| n =   11 | C = 8.859e-03 | score: 0.669 |\n",
      "\t| n =   12 | C = 8.859e-03 | score: 0.665 |\n",
      "\t| n =   14 | C = 8.859e-03 | score: 0.668 |\n",
      "\t| n =   16 | C = 4.281e-01 | score: 0.680 |\n",
      "\t| n =   18 | C = 6.158e-02 | score: 0.690 |\n",
      "\t| n =   20 | C = 3.360e-03 | score: 0.691 |\n",
      "\t| n =   23 | C = 3.360e-03 | score: 0.691 |\n",
      "\t| n =   27 | C = 1.274e-03 | score: 0.694 |\n",
      "\t| n =   30 | C = 3.360e-03 | score: 0.691 |\n",
      "\t| n =   34 | C = 1.274e-03 | score: 0.689 |\n",
      "\t| n =   39 | C = 1.274e-03 | score: 0.688 |\n",
      "\t| n =   44 | C = 8.859e-03 | score: 0.693 |\n",
      "\t| n =   50 | C = 8.859e-03 | score: 0.695 |\n",
      "\t| n =   57 | C = 6.158e-02 | score: 0.694 |\n",
      "\t| n =   65 | C = 3.360e-03 | score: 0.689 |\n",
      "\t| n =   74 | C = 2.336e-02 | score: 0.687 |\n",
      "\t| n =   84 | C = 2.336e-02 | score: 0.688 |\n",
      "\t| n =   96 | C = 3.360e-03 | score: 0.693 |\n",
      "\t| n =  109 | C = 8.859e-03 | score: 0.693 |\n",
      "\t| n =  123 | C = 3.360e-03 | score: 0.691 |\n",
      "\t| n =  140 | C = 3.360e-03 | score: 0.690 |\n",
      "\t| n =  159 | C = 1.274e-03 | score: 0.692 |\n",
      "\t| n =  181 | C = 3.360e-03 | score: 0.696 |\n",
      "\t| n =  205 | C = 3.360e-03 | score: 0.697 |\n",
      "\t| n =  233 | C = 3.360e-03 | score: 0.699 |\n",
      "\t| n =  265 | C = 3.360e-03 | score: 0.699 |\n",
      "\t| n =  301 | C = 3.360e-03 | score: 0.702 |\n",
      "\t| n =  341 | C = 3.360e-03 | score: 0.699 |\n",
      "\t| n =  388 | C = 3.360e-03 | score: 0.706 |\n",
      "\t| n =  440 | C = 3.360e-03 | score: 0.709 |\n",
      "\t| n =  500 | C = 3.360e-03 | score: 0.706 |\n",
      "\tBest score was 0.709 with C = 3.36e-03 and n = 440\n",
      "Using solver:  saga\n",
      "\t| n =    1 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| n =    2 | C = 2.336e-02 | score: 0.613 |\n",
      "\t| n =    3 | C = 1.833e-04 | score: 0.631 |\n",
      "\t| n =    4 | C = 1.833e-04 | score: 0.639 |\n",
      "\t| n =    5 | C = 8.859e-03 | score: 0.648 |\n",
      "\t| n =    6 | C = 3.360e-03 | score: 0.658 |\n",
      "\t| n =    7 | C = 3.360e-03 | score: 0.665 |\n",
      "\t| n =    8 | C = 3.360e-03 | score: 0.666 |\n",
      "\t| n =    9 | C = 2.976e+00 | score: 0.664 |\n",
      "\t| n =   11 | C = 8.859e-03 | score: 0.669 |\n",
      "\t| n =   12 | C = 8.859e-03 | score: 0.664 |\n",
      "\t| n =   14 | C = 8.859e-03 | score: 0.668 |\n",
      "\t| n =   16 | C = 1.624e-01 | score: 0.680 |\n",
      "\t| n =   18 | C = 6.158e-02 | score: 0.689 |\n",
      "\t| n =   20 | C = 3.360e-03 | score: 0.691 |\n",
      "\t| n =   23 | C = 1.274e-03 | score: 0.691 |\n",
      "\t| n =   27 | C = 1.274e-03 | score: 0.693 |\n",
      "\t| n =   30 | C = 3.360e-03 | score: 0.691 |\n",
      "\t| n =   34 | C = 1.274e-03 | score: 0.689 |\n",
      "\t| n =   39 | C = 1.274e-03 | score: 0.688 |\n",
      "\t| n =   44 | C = 8.859e-03 | score: 0.693 |\n",
      "\t| n =   50 | C = 8.859e-03 | score: 0.695 |\n",
      "\t| n =   57 | C = 6.158e-02 | score: 0.694 |\n",
      "\t| n =   65 | C = 3.360e-03 | score: 0.688 |\n",
      "\t| n =   74 | C = 2.336e-02 | score: 0.687 |\n",
      "\t| n =   84 | C = 2.336e-02 | score: 0.688 |\n",
      "\t| n =   96 | C = 3.360e-03 | score: 0.693 |\n",
      "\t| n =  109 | C = 8.859e-03 | score: 0.693 |\n",
      "\t| n =  123 | C = 3.360e-03 | score: 0.691 |\n",
      "\t| n =  140 | C = 3.360e-03 | score: 0.690 |\n",
      "\t| n =  159 | C = 3.360e-03 | score: 0.691 |\n",
      "\t| n =  181 | C = 3.360e-03 | score: 0.697 |\n",
      "\t| n =  205 | C = 3.360e-03 | score: 0.697 |\n",
      "\t| n =  233 | C = 3.360e-03 | score: 0.699 |\n",
      "\t| n =  265 | C = 3.360e-03 | score: 0.699 |\n",
      "\t| n =  301 | C = 3.360e-03 | score: 0.702 |\n",
      "\t| n =  341 | C = 3.360e-03 | score: 0.699 |\n",
      "\t| n =  388 | C = 3.360e-03 | score: 0.706 |\n",
      "\t| n =  440 | C = 3.360e-03 | score: 0.709 |\n",
      "\t| n =  500 | C = 3.360e-03 | score: 0.706 |\n",
      "\tBest score was 0.709 with C = 3.36e-03 and n = 440\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(5, shuffle=True, random_state=0)\n",
    "solvers = ['liblinear', 'lbfgs', 'sag', 'saga']\n",
    "C_vals = np.logspace(-5, 3, 20)\n",
    "bestSol   = None\n",
    "bestScore = 0\n",
    "bestN = 0\n",
    "bestC = 0\n",
    "\n",
    "solvDict = {}\n",
    "for solver in solvers:\n",
    "    print(\"Using solver: \", solver)\n",
    "    all_scores = []\n",
    "    for n in n_vals:\n",
    "        c_scores = []\n",
    "        for C in C_vals:\n",
    "            scores = []\n",
    "            for train, test in kf.split(sscaledX_train):\n",
    "                logRK = LogisticRegression(C=C, solver=solver)\n",
    "                PCAn = PCA(int(n), random_state=1000).fit(sscaledX_train[train])\n",
    "                X_new = PCAn.transform(sscaledX_train[train])\n",
    "                X_val_new = PCAn.transform(sscaledX_train[test])\n",
    "\n",
    "                logRK.fit(X_new, y_train.iloc[train])\n",
    "                scores.append(logRK.score(X_val_new, y_train.iloc[test]))\n",
    "\n",
    "            c_scores.append(np.mean(scores))\n",
    "        currbestC = np.argmax(c_scores)\n",
    "        all_scores.append( (C_vals[currbestC], c_scores[currbestC]) )\n",
    "        print(\"\\t| n = {0:>4} | C = {1:2.3e} | score: {2:.3f} |\".format(int(n), C_vals[currbestC], c_scores[currbestC]))\n",
    "        \n",
    "    solvDict[solver] = all_scores\n",
    "    means = [ x[1] for x in all_scores ]\n",
    "    best = np.argmax(means)\n",
    "    if means[best] > bestScore:\n",
    "        bestScore = means[best]\n",
    "        bestSol = solver\n",
    "        bestN = n_vals[best]\n",
    "        bestC = all_scores[best][0]\n",
    "    print(\"\\tBest score was {0:.3f} with C = {1:3.2e} and n = {2}\".format(means[best], all_scores[best][0], n_vals[best]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lbfgs 6 440\n",
      "0.6954177897574124\n"
     ]
    }
   ],
   "source": [
    "print(bestSol, bestC, bestN)\n",
    "logRSC = LogisticRegression(C=0.00336, solver=bestSol)\n",
    "PCAn = PCA(bestN, random_state=1000).fit(sscaledX_train)\n",
    "X_new = PCAn.transform(sscaledX_train)\n",
    "X_val_new = PCAn.transform(sscaledX_val)\n",
    "\n",
    "logRSC.fit(X_new, y_train)\n",
    "\n",
    "print( logRSC.score(X_val_new, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. The Min-Max scaled Data\n",
    "(the same as above but with mmscaledX_train rather than sscaledX_train; I kept both runs for clarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using solver:  lbfgs\n",
      "\t| n =    1 | C = 1.129e+00 | score: 0.572 |\n",
      "\t| n =    2 | C = 4.281e-01 | score: 0.632 |\n",
      "\t| n =    3 | C = 3.360e-03 | score: 0.640 |\n",
      "\t| n =    4 | C = 3.360e-03 | score: 0.635 |\n",
      "\t| n =    5 | C = 1.129e+00 | score: 0.645 |\n",
      "\t| n =    6 | C = 1.624e-01 | score: 0.665 |\n",
      "\t| n =    7 | C = 6.158e-02 | score: 0.678 |\n",
      "\t| n =    8 | C = 4.281e-01 | score: 0.672 |\n",
      "\t| n =    9 | C = 4.281e-01 | score: 0.672 |\n",
      "\t| n =   11 | C = 1.624e-01 | score: 0.668 |\n",
      "\t| n =   12 | C = 4.281e-01 | score: 0.668 |\n",
      "\t| n =   14 | C = 6.158e-02 | score: 0.673 |\n",
      "\t| n =   16 | C = 5.456e+01 | score: 0.671 |\n",
      "\t| n =   18 | C = 6.158e-02 | score: 0.681 |\n",
      "\t| n =   20 | C = 1.624e-01 | score: 0.684 |\n",
      "\t| n =   23 | C = 6.158e-02 | score: 0.685 |\n",
      "\t| n =   27 | C = 2.976e+00 | score: 0.688 |\n",
      "\t| n =   30 | C = 1.624e-01 | score: 0.685 |\n",
      "\t| n =   34 | C = 4.281e-01 | score: 0.688 |\n",
      "\t| n =   39 | C = 1.624e-01 | score: 0.694 |\n",
      "\t| n =   44 | C = 7.848e+00 | score: 0.694 |\n",
      "\t| n =   50 | C = 1.624e-01 | score: 0.694 |\n",
      "\t| n =   57 | C = 4.281e-01 | score: 0.694 |\n",
      "\t| n =   65 | C = 1.129e+00 | score: 0.697 |\n",
      "\t| n =   74 | C = 4.281e-01 | score: 0.702 |\n",
      "\t| n =   84 | C = 1.624e-01 | score: 0.699 |\n",
      "\t| n =   96 | C = 6.158e-02 | score: 0.690 |\n",
      "\t| n =  109 | C = 1.624e-01 | score: 0.690 |\n",
      "\t| n =  123 | C = 6.158e-02 | score: 0.692 |\n",
      "\t| n =  140 | C = 1.624e-01 | score: 0.692 |\n",
      "\t| n =  159 | C = 1.624e-01 | score: 0.690 |\n",
      "\t| n =  181 | C = 4.281e-01 | score: 0.690 |\n",
      "\t| n =  205 | C = 1.624e-01 | score: 0.697 |\n",
      "\t| n =  233 | C = 1.624e-01 | score: 0.698 |\n",
      "\t| n =  265 | C = 1.624e-01 | score: 0.701 |\n",
      "\t| n =  301 | C = 1.624e-01 | score: 0.702 |\n",
      "\t| n =  341 | C = 1.624e-01 | score: 0.697 |\n",
      "\t| n =  388 | C = 1.624e-01 | score: 0.703 |\n",
      "\t| n =  440 | C = 1.624e-01 | score: 0.704 |\n",
      "\t| n =  500 | C = 1.624e-01 | score: 0.704 |\n",
      "\tBest score was 0.704 with C = 1.62e-01 and n = 440\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(5, shuffle=True, random_state=0)\n",
    "solvers = ['lbfgs']\n",
    "C_vals = np.logspace(-5, 3, 20)\n",
    "mBestSol   = None\n",
    "mBestScore = 0\n",
    "mBestN = 0\n",
    "mBestC = 0\n",
    "\n",
    "mSolvDict = {}\n",
    "for solver in solvers:\n",
    "    print(\"Using solver: \", solver)\n",
    "    all_scores = []\n",
    "    for n in n_vals:\n",
    "        c_scores = []\n",
    "        for C in C_vals:\n",
    "            scores = []\n",
    "            for train, test in kf.split(mmscaledX_train):\n",
    "                logRK = LogisticRegression(C=C, solver=solver)\n",
    "                PCAn = PCA(int(n), random_state=1000).fit(mmscaledX_train[train])\n",
    "                X_new = PCAn.transform(mmscaledX_train[train])\n",
    "                X_val_new = PCAn.transform(mmscaledX_train[test])\n",
    "\n",
    "                logRK.fit(X_new, y_train.iloc[train])\n",
    "                scores.append(logRK.score(X_val_new, y_train.iloc[test]))\n",
    "\n",
    "            c_scores.append(np.mean(scores))\n",
    "        currbestC = np.argmax(c_scores)\n",
    "        all_scores.append( (C_vals[currbestC], c_scores[currbestC]) )\n",
    "        print(\"\\t| n = {0:>4} | C = {1:2.3e} | score: {2:.3f} |\".format(int(n), C_vals[currbestC], c_scores[currbestC]))\n",
    "        \n",
    "    mSolvDict[solver] = all_scores\n",
    "    means = [ x[1] for x in all_scores ]\n",
    "    best = np.argmax(means)\n",
    "    if means[best] > mBestScore:\n",
    "        mBestScore = means[best]\n",
    "        mBestSol = solver\n",
    "        mBestN = n_vals[best]\n",
    "        mBestC = all_scores[best][0]\n",
    "    print(\"\\tBest score was {0:.3f} with C = {1:3.2e} and n = {2}\".format(means[best], all_scores[best][0], n_vals[best]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lbfgs 0.1623776739188721 440\n",
      "0.7070979335130279\n"
     ]
    }
   ],
   "source": [
    "print(mBestSol, mBestC, mBestN)\n",
    "logRSC = LogisticRegression(C=mBestC, solver=mBestSol)\n",
    "PCAn = PCA(bestN, random_state=1000).fit(mmscaledX_train)\n",
    "X_new = PCAn.transform(mmscaledX_train)\n",
    "X_val_new = PCAn.transform(mmscaledX_val)\n",
    "\n",
    "logRSC.fit(X_new, y_train)\n",
    "\n",
    "print( logRSC.score(X_val_new, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5838181517935674"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logRSC = LogisticRegression(C=0.1623776739188721, solver='lbfgs')\n",
    "PCAn = PCA(440, random_state=1000).fit(mmscaledX_train)\n",
    "X_new = PCAn.transform(mmscaledX_train)\n",
    "X_val_new = PCAn.transform(mmscaledX_val)\n",
    "logRSC.fit(X_new, y_train)\n",
    "\n",
    "logrProb = logRSC.predict_proba(X_val_new)\n",
    "log_loss(y_val, logrProb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The MinMax scaling of the data helps us get a little better accuracy on the validation set, but still about the same as the random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| n =    1 | C = 1.000e-05 | score: 0.551 |\n",
      "| n =    2 | C = 1.292e-02 | score: 0.611 |\n",
      "| n =    3 | C = 2.154e-04 | score: 0.628 |\n",
      "| n =    5 | C = 3.594e-02 | score: 0.650 |\n",
      "| n =    7 | C = 4.642e-03 | score: 0.665 |\n",
      "| n =    9 | C = 4.642e-03 | score: 0.665 |\n",
      "| n =   13 | C = 4.642e-03 | score: 0.664 |\n",
      "| n =   18 | C = 4.642e-03 | score: 0.694 |\n",
      "| n =   26 | C = 3.594e-02 | score: 0.691 |\n",
      "| n =   36 | C = 1.668e-03 | score: 0.687 |\n",
      "| n =   50 | C = 5.995e-04 | score: 0.686 |\n",
      "| n =   70 | C = 3.594e-02 | score: 0.698 |\n",
      "| n =   97 | C = 1.668e-03 | score: 0.695 |\n",
      "| n =  135 | C = 5.995e-04 | score: 0.694 |\n",
      "| n =  187 | C = 5.995e-04 | score: 0.701 |\n",
      "| n =  259 | C = 5.995e-04 | score: 0.702 |\n",
      "| n =  360 | C = 5.995e-04 | score: 0.700 |\n",
      "| n =  500 | C = 5.995e-04 | score: 0.698 |\n",
      "\tBest score was 0.702 with C = 5.99e-04 and n = 259\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(5, shuffle=True, random_state=0)\n",
    "C_vals = np.logspace(-5, -1, 10)\n",
    "n_vals = np.logspace(0, 2.699, 20)\n",
    "n_vals = list(sorted(set([ int(n) for n in n_vals ])))\n",
    "bestScore = 0\n",
    "bestN = 0\n",
    "bestC = 0\n",
    "\n",
    "all_scores = []\n",
    "for n in n_vals:\n",
    "    c_scores = []\n",
    "    for C in C_vals:\n",
    "        scores = []\n",
    "        for train, test in kf.split(sscaledX_train):\n",
    "            SVM = SVC(C=C, kernel='linear')\n",
    "            PCAn = PCA(n, random_state=1000).fit(sscaledX_train[train])\n",
    "            X_new = PCAn.transform(sscaledX_train[train])\n",
    "            X_val_new = PCAn.transform(sscaledX_train[test])\n",
    "\n",
    "            SVM.fit(X_new, y_train.iloc[train])\n",
    "            scores.append(SVM.score(X_val_new, y_train.iloc[test]))\n",
    "\n",
    "        c_scores.append(np.mean(scores))\n",
    "    currbestC = np.argmax(c_scores)\n",
    "    all_scores.append( (C_vals[currbestC], c_scores[currbestC]) )\n",
    "    print(\"| n = {0:>4} | C = {1:2.3e} | score: {2:.3f} |\".format(int(n), C_vals[currbestC], c_scores[currbestC]))\n",
    "\n",
    "means = [ x[1] for x in all_scores ]\n",
    "best = np.argmax(means)\n",
    "if means[best] > bestScore:\n",
    "    bestScore = means[best]\n",
    "    bestN = n_vals[best]\n",
    "    bestC = all_scores[best][0]\n",
    "print(\"\\tBest score was {0:.3f} with C = {1:3.2e} and n = {2}\".format(means[best], all_scores[best][0], n_vals[best]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0005994842503189409 259\n",
      "0.6918238993710691\n"
     ]
    }
   ],
   "source": [
    "print(bestC, bestN)\n",
    "SVM = SVC(C=mBestC, kernel='linear')\n",
    "PCAn = PCA(bestN, random_state=1000).fit(sscaledX_train)\n",
    "X_new = PCAn.transform(sscaledX_train)\n",
    "X_val_new = PCAn.transform(sscaledX_val)\n",
    "\n",
    "SVM.fit(X_new, y_train)\n",
    "\n",
    "print( SVM.score(X_val_new, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MinMax data scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| n =    1 | C = 1.000e-05 | score: 0.551 |\n",
      "| n =    2 | C = 1.000e-01 | score: 0.605 |\n",
      "| n =    3 | C = 5.995e-04 | score: 0.623 |\n",
      "| n =    5 | C = 5.995e-04 | score: 0.607 |\n",
      "| n =    7 | C = 1.668e-03 | score: 0.641 |\n",
      "| n =    9 | C = 5.995e-04 | score: 0.647 |\n",
      "| n =   13 | C = 5.995e-04 | score: 0.662 |\n",
      "| n =   18 | C = 5.995e-04 | score: 0.673 |\n",
      "| n =   26 | C = 5.995e-04 | score: 0.667 |\n",
      "| n =   36 | C = 5.995e-04 | score: 0.678 |\n",
      "| n =   50 | C = 5.995e-04 | score: 0.668 |\n",
      "| n =   70 | C = 5.995e-04 | score: 0.676 |\n",
      "| n =   97 | C = 5.995e-04 | score: 0.667 |\n",
      "| n =  135 | C = 5.995e-04 | score: 0.670 |\n",
      "| n =  187 | C = 5.995e-04 | score: 0.664 |\n",
      "| n =  259 | C = 5.995e-04 | score: 0.656 |\n",
      "| n =  360 | C = 5.995e-04 | score: 0.650 |\n",
      "| n =  500 | C = 5.995e-04 | score: 0.659 |\n",
      "\tBest score was 0.678 with C = 5.99e-04 and n = 36\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(5, shuffle=True, random_state=0)\n",
    "C_vals = np.logspace(-5, -1, 10)\n",
    "n_vals = np.logspace(0, 2.699, 20)\n",
    "n_vals = list(sorted(set([ int(n) for n in n_vals ])))\n",
    "mBestScore = 0\n",
    "mBestN = 0\n",
    "mBestC = 0\n",
    "\n",
    "all_scores = []\n",
    "for n in n_vals:\n",
    "    c_scores = []\n",
    "    for C in C_vals:\n",
    "        scores = []\n",
    "        for train, test in kf.split(mmscaledX_train):\n",
    "            SVM = SVC(C=C, kernel='linear')\n",
    "            PCAn = PCA(n, random_state=1000).fit(mmscaledX_train[train])\n",
    "            X_new = PCAn.transform(sscaledX_train[train])\n",
    "            X_val_new = PCAn.transform(mmscaledX_train[test])\n",
    "\n",
    "            SVM.fit(X_new, y_train.iloc[train])\n",
    "            scores.append(SVM.score(X_val_new, y_train.iloc[test]))\n",
    "\n",
    "        c_scores.append(np.mean(scores))\n",
    "    currbestC = np.argmax(c_scores)\n",
    "    all_scores.append( (C_vals[currbestC], c_scores[currbestC]) )\n",
    "    print(\"| n = {0:>4} | C = {1:2.3e} | score: {2:.3f} |\".format(int(n), C_vals[currbestC], c_scores[currbestC]))\n",
    "\n",
    "means = [ x[1] for x in all_scores ]\n",
    "best = np.argmax(means)\n",
    "if means[best] > mBestScore:\n",
    "    mBestScore = means[best]\n",
    "    mBestN = n_vals[best]\n",
    "    mBestC = all_scores[best][0]\n",
    "print(\"\\tBest score was {0:.3f} with C = {1:3.2e} and n = {2}\".format(means[best], all_scores[best][0], n_vals[best]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0005994842503189409 36\n",
      "0.5265049415992812\n"
     ]
    }
   ],
   "source": [
    "print(mBestC, mBestN)\n",
    "SVM = SVC(C=mBestC, kernel='linear')\n",
    "PCAn = PCA(mBestN, random_state=1000).fit(mmscaledX_train)\n",
    "X_new = PCAn.transform(mmscaledX_train)\n",
    "X_val_new = PCAn.transform(mmscaledX_val)\n",
    "\n",
    "SVM.fit(X_new, y_train)\n",
    "\n",
    "print( SVM.score(X_val_new, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Degree 2\n",
      "\t| n =    1 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| n =    2 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| n =    3 | C = 7.743e-05 | score: 0.567 |\n",
      "\t| n =    5 | C = 4.642e-03 | score: 0.612 |\n",
      "\t| n =    7 | C = 5.995e-04 | score: 0.634 |\n",
      "\t| n =    9 | C = 1.668e-03 | score: 0.648 |\n",
      "\t| n =   13 | C = 1.292e-02 | score: 0.652 |\n",
      "\t| n =   18 | C = 1.668e-03 | score: 0.651 |\n",
      "\t| n =   26 | C = 1.668e-03 | score: 0.655 |\n",
      "\t| n =   36 | C = 4.642e-03 | score: 0.662 |\n",
      "\t| n =   50 | C = 4.642e-03 | score: 0.656 |\n",
      "\t| n =   70 | C = 1.292e-02 | score: 0.666 |\n",
      "\t| n =   97 | C = 3.594e-02 | score: 0.668 |\n",
      "\t| n =  135 | C = 3.594e-02 | score: 0.667 |\n",
      "\t| n =  187 | C = 1.000e-01 | score: 0.671 |\n",
      "\t| n =  259 | C = 1.000e-01 | score: 0.660 |\n",
      "\t| n =  360 | C = 1.000e-01 | score: 0.599 |\n",
      "\t| n =  500 | C = 1.000e-01 | score: 0.575 |\n",
      "\tBest score was 0.671 with C = 1.00e-01 and n = 187\n",
      "Degree 3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-123-e3d9d1333dba>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m                 \u001b[0mX_val_new\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPCAn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msscaledX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m                 \u001b[0mSVM\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_new\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m                 \u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSVM\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val_new\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\sklearn\\svm\\base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m         \u001b[0mseed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrnd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'i'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 212\u001b[1;33m         \u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    213\u001b[0m         \u001b[1;31m# see comment on the other call to np.iinfo in this file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\sklearn\\svm\\base.py\u001b[0m in \u001b[0;36m_dense_fit\u001b[1;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[0;32m    269\u001b[0m                 \u001b[0mcache_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcache_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcoef0\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m                 \u001b[0mgamma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 271\u001b[1;33m                 max_iter=self.max_iter, random_seed=random_seed)\n\u001b[0m\u001b[0;32m    272\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    273\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_warn_from_fit_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "kf = KFold(5, shuffle=True, random_state=0)\n",
    "C_vals = np.logspace(-5, -1, 10)\n",
    "n_vals = np.logspace(0, 2.699, 20)\n",
    "n_vals = list(sorted(set([ int(n) for n in n_vals ])))\n",
    "bestScore = 0\n",
    "bestN = 0\n",
    "bestC = 0\n",
    "bestD = 0\n",
    "\n",
    "for d in range(2, 4):\n",
    "    print(\"Degree\", d)\n",
    "    all_scores = []\n",
    "    for n in n_vals:\n",
    "        c_scores = []\n",
    "        for C in C_vals:\n",
    "            scores = []\n",
    "            for train, test in kf.split(sscaledX_train):\n",
    "                SVM = SVC(C=C, kernel='poly', degree=d)\n",
    "                PCAn = PCA(n, random_state=1000).fit(sscaledX_train[train])\n",
    "                X_new = PCAn.transform(sscaledX_train[train])\n",
    "                X_val_new = PCAn.transform(sscaledX_train[test])\n",
    "\n",
    "                SVM.fit(X_new, y_train.iloc[train])\n",
    "                scores.append(SVM.score(X_val_new, y_train.iloc[test]))\n",
    "\n",
    "            c_scores.append(np.mean(scores))\n",
    "        bestC = np.argmax(c_scores)\n",
    "        all_scores.append( (C_vals[bestC], c_scores[bestC]) )\n",
    "        print(\"\\t| n = {0:>4} | C = {1:2.3e} | score: {2:.3f} |\".format(int(n), C_vals[bestC], c_scores[bestC]))\n",
    "\n",
    "    means = [ x[1] for x in all_scores ]\n",
    "    best = np.argmax(means)\n",
    "    if means[best] > bestScore:\n",
    "        bestScore = means[best]\n",
    "        bestD = d\n",
    "        bestN = n_vals[best]\n",
    "        bestC = all_scores[best][0]\n",
    "    print(\"\\tBest score was {0:.3f} with C = {1:3.2e} and n = {2}\".format(means[best], all_scores[best][0], n_vals[best]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1 187 2\n",
      "0.5265049415992812\n"
     ]
    }
   ],
   "source": [
    "print(bestC, bestN, bestD)\n",
    "SVM = SVC(C=mBestC, kernel='poly', degree=bestD)\n",
    "PCAn = PCA(bestN, random_state=1000).fit(sscaledX_train)\n",
    "X_new = PCAn.transform(sscaledX_train)\n",
    "X_val_new = PCAn.transform(sscaledX_val)\n",
    "\n",
    "SVM.fit(X_new, y_train)\n",
    "\n",
    "print( SVM.score(X_val_new, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Radial Basis Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n = 1\n",
      "\t| g = 1.000e-05 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 2.336e-05 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 5.456e-05 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 1.274e-04 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 2.976e-04 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 6.952e-04 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 1.624e-03 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 3.793e-03 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 8.859e-03 | C = 1.000e-01 | score: 0.564 |\n",
      "\t| g = 2.069e-02 | C = 1.000e-01 | score: 0.564 |\n",
      "\t| g = 4.833e-02 | C = 1.000e-01 | score: 0.566 |\n",
      "\t| g = 1.129e-01 | C = 1.000e-01 | score: 0.571 |\n",
      "\t| g = 2.637e-01 | C = 1.000e-01 | score: 0.570 |\n",
      "\t| g = 6.158e-01 | C = 1.000e-01 | score: 0.568 |\n",
      "\t| g = 1.438e+00 | C = 1.000e-01 | score: 0.564 |\n",
      "\t| g = 3.360e+00 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 7.848e+00 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 1.833e+01 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 4.281e+01 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 1.000e+02 | C = 1.000e-05 | score: 0.551 |\n",
      "\tBest score was 0.571 with C = 1.00e-01 and g = 0.11288378916846883\n",
      "n = 2\n",
      "\t| g = 1.000e-05 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 2.336e-05 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 5.456e-05 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 1.274e-04 | C = 1.000e-01 | score: 0.552 |\n",
      "\t| g = 2.976e-04 | C = 1.000e-01 | score: 0.586 |\n",
      "\t| g = 6.952e-04 | C = 1.000e-01 | score: 0.613 |\n",
      "\t| g = 1.624e-03 | C = 1.000e-01 | score: 0.613 |\n",
      "\t| g = 3.793e-03 | C = 1.000e-01 | score: 0.623 |\n",
      "\t| g = 8.859e-03 | C = 1.000e-01 | score: 0.622 |\n",
      "\t| g = 2.069e-02 | C = 1.000e-01 | score: 0.625 |\n",
      "\t| g = 4.833e-02 | C = 1.000e-01 | score: 0.632 |\n",
      "\t| g = 1.129e-01 | C = 1.000e-01 | score: 0.627 |\n",
      "\t| g = 2.637e-01 | C = 1.000e-01 | score: 0.611 |\n",
      "\t| g = 6.158e-01 | C = 1.000e-01 | score: 0.566 |\n",
      "\t| g = 1.438e+00 | C = 1.000e-01 | score: 0.553 |\n",
      "\t| g = 3.360e+00 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 7.848e+00 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 1.833e+01 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 4.281e+01 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 1.000e+02 | C = 1.000e-05 | score: 0.551 |\n",
      "\tBest score was 0.632 with C = 1.00e-01 and g = 0.04832930238571752\n",
      "n = 3\n",
      "\t| g = 1.000e-05 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 2.336e-05 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 5.456e-05 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 1.274e-04 | C = 1.000e-01 | score: 0.553 |\n",
      "\t| g = 2.976e-04 | C = 1.000e-01 | score: 0.601 |\n",
      "\t| g = 6.952e-04 | C = 1.000e-01 | score: 0.629 |\n",
      "\t| g = 1.624e-03 | C = 1.000e-01 | score: 0.636 |\n",
      "\t| g = 3.793e-03 | C = 1.000e-01 | score: 0.647 |\n",
      "\t| g = 8.859e-03 | C = 1.000e-01 | score: 0.653 |\n",
      "\t| g = 2.069e-02 | C = 1.000e-01 | score: 0.650 |\n",
      "\t| g = 4.833e-02 | C = 1.000e-01 | score: 0.639 |\n",
      "\t| g = 1.129e-01 | C = 1.000e-01 | score: 0.611 |\n",
      "\t| g = 2.637e-01 | C = 1.000e-01 | score: 0.555 |\n",
      "\t| g = 6.158e-01 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 1.438e+00 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 3.360e+00 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 7.848e+00 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 1.833e+01 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 4.281e+01 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 1.000e+02 | C = 1.000e-05 | score: 0.551 |\n",
      "\tBest score was 0.653 with C = 1.00e-01 and g = 0.008858667904100823\n",
      "n = 5\n",
      "\t| g = 1.000e-05 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 2.336e-05 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 5.456e-05 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 1.274e-04 | C = 1.000e-01 | score: 0.553 |\n",
      "\t| g = 2.976e-04 | C = 1.000e-01 | score: 0.622 |\n",
      "\t| g = 6.952e-04 | C = 1.000e-01 | score: 0.646 |\n",
      "\t| g = 1.624e-03 | C = 1.000e-01 | score: 0.656 |\n",
      "\t| g = 3.793e-03 | C = 1.000e-01 | score: 0.657 |\n",
      "\t| g = 8.859e-03 | C = 1.000e-01 | score: 0.668 |\n",
      "\t| g = 2.069e-02 | C = 1.000e-01 | score: 0.659 |\n",
      "\t| g = 4.833e-02 | C = 1.000e-01 | score: 0.608 |\n",
      "\t| g = 1.129e-01 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 2.637e-01 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 6.158e-01 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 1.438e+00 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 3.360e+00 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 7.848e+00 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 1.833e+01 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 4.281e+01 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 1.000e+02 | C = 1.000e-05 | score: 0.551 |\n",
      "\tBest score was 0.668 with C = 1.00e-01 and g = 0.008858667904100823\n",
      "n = 7\n",
      "\t| g = 1.000e-05 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 2.336e-05 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 5.456e-05 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 1.274e-04 | C = 1.000e-01 | score: 0.560 |\n",
      "\t| g = 2.976e-04 | C = 1.000e-01 | score: 0.643 |\n",
      "\t| g = 6.952e-04 | C = 1.000e-01 | score: 0.660 |\n",
      "\t| g = 1.624e-03 | C = 1.000e-01 | score: 0.667 |\n",
      "\t| g = 3.793e-03 | C = 1.000e-01 | score: 0.680 |\n",
      "\t| g = 8.859e-03 | C = 1.000e-01 | score: 0.682 |\n",
      "\t| g = 2.069e-02 | C = 1.000e-01 | score: 0.669 |\n",
      "\t| g = 4.833e-02 | C = 1.000e-01 | score: 0.567 |\n",
      "\t| g = 1.129e-01 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 2.637e-01 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 6.158e-01 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 1.438e+00 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 3.360e+00 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 7.848e+00 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 1.833e+01 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 4.281e+01 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 1.000e+02 | C = 1.000e-05 | score: 0.551 |\n",
      "\tBest score was 0.682 with C = 1.00e-01 and g = 0.008858667904100823\n",
      "n = 9\n",
      "\t| g = 1.000e-05 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 2.336e-05 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 5.456e-05 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 1.274e-04 | C = 1.000e-01 | score: 0.560 |\n",
      "\t| g = 2.976e-04 | C = 1.000e-01 | score: 0.643 |\n",
      "\t| g = 6.952e-04 | C = 1.000e-01 | score: 0.657 |\n",
      "\t| g = 1.624e-03 | C = 1.000e-01 | score: 0.671 |\n",
      "\t| g = 3.793e-03 | C = 1.000e-01 | score: 0.684 |\n",
      "\t| g = 8.859e-03 | C = 1.000e-01 | score: 0.691 |\n",
      "\t| g = 2.069e-02 | C = 1.000e-01 | score: 0.657 |\n",
      "\t| g = 4.833e-02 | C = 1.000e-01 | score: 0.552 |\n",
      "\t| g = 1.129e-01 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 2.637e-01 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 6.158e-01 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 1.438e+00 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 3.360e+00 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 7.848e+00 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 1.833e+01 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 4.281e+01 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 1.000e+02 | C = 1.000e-05 | score: 0.551 |\n",
      "\tBest score was 0.691 with C = 1.00e-01 and g = 0.008858667904100823\n",
      "n = 13\n",
      "\t| g = 1.000e-05 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 2.336e-05 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 5.456e-05 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 1.274e-04 | C = 1.000e-01 | score: 0.560 |\n",
      "\t| g = 2.976e-04 | C = 1.000e-01 | score: 0.640 |\n",
      "\t| g = 6.952e-04 | C = 1.000e-01 | score: 0.658 |\n",
      "\t| g = 1.624e-03 | C = 1.000e-01 | score: 0.676 |\n",
      "\t| g = 3.793e-03 | C = 1.000e-01 | score: 0.693 |\n",
      "\t| g = 8.859e-03 | C = 1.000e-01 | score: 0.697 |\n",
      "\t| g = 2.069e-02 | C = 1.000e-01 | score: 0.630 |\n",
      "\t| g = 4.833e-02 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 1.129e-01 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 2.637e-01 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 6.158e-01 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 1.438e+00 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 3.360e+00 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 7.848e+00 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 1.833e+01 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 4.281e+01 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 1.000e+02 | C = 1.000e-05 | score: 0.551 |\n",
      "\tBest score was 0.697 with C = 1.00e-01 and g = 0.008858667904100823\n",
      "n = 18\n",
      "\t| g = 1.000e-05 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 2.336e-05 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 5.456e-05 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 1.274e-04 | C = 1.000e-01 | score: 0.562 |\n",
      "\t| g = 2.976e-04 | C = 1.000e-01 | score: 0.647 |\n",
      "\t| g = 6.952e-04 | C = 1.000e-01 | score: 0.670 |\n",
      "\t| g = 1.624e-03 | C = 1.000e-01 | score: 0.687 |\n",
      "\t| g = 3.793e-03 | C = 1.000e-01 | score: 0.697 |\n",
      "\t| g = 8.859e-03 | C = 1.000e-01 | score: 0.697 |\n",
      "\t| g = 2.069e-02 | C = 1.000e-01 | score: 0.592 |\n",
      "\t| g = 4.833e-02 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 1.129e-01 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 2.637e-01 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 6.158e-01 | C = 1.000e-05 | score: 0.551 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t| g = 1.438e+00 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 3.360e+00 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 7.848e+00 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 1.833e+01 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 4.281e+01 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 1.000e+02 | C = 1.000e-05 | score: 0.551 |\n",
      "\tBest score was 0.697 with C = 1.00e-01 and g = 0.008858667904100823\n",
      "n = 26\n",
      "\t| g = 1.000e-05 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 2.336e-05 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 5.456e-05 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 1.274e-04 | C = 1.000e-01 | score: 0.563 |\n",
      "\t| g = 2.976e-04 | C = 1.000e-01 | score: 0.656 |\n",
      "\t| g = 6.952e-04 | C = 1.000e-01 | score: 0.673 |\n",
      "\t| g = 1.624e-03 | C = 1.000e-01 | score: 0.690 |\n",
      "\t| g = 3.793e-03 | C = 1.000e-01 | score: 0.703 |\n",
      "\t| g = 8.859e-03 | C = 1.000e-01 | score: 0.691 |\n",
      "\t| g = 2.069e-02 | C = 1.000e-01 | score: 0.561 |\n",
      "\t| g = 4.833e-02 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 1.129e-01 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 2.637e-01 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 6.158e-01 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 1.438e+00 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 3.360e+00 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 7.848e+00 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 1.833e+01 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 4.281e+01 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 1.000e+02 | C = 1.000e-05 | score: 0.551 |\n",
      "\tBest score was 0.703 with C = 1.00e-01 and g = 0.00379269019073225\n",
      "n = 36\n",
      "\t| g = 1.000e-05 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 2.336e-05 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 5.456e-05 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 1.274e-04 | C = 1.000e-01 | score: 0.562 |\n",
      "\t| g = 2.976e-04 | C = 1.000e-01 | score: 0.656 |\n",
      "\t| g = 6.952e-04 | C = 1.000e-01 | score: 0.671 |\n",
      "\t| g = 1.624e-03 | C = 1.000e-01 | score: 0.690 |\n",
      "\t| g = 3.793e-03 | C = 1.000e-01 | score: 0.701 |\n",
      "\t| g = 8.859e-03 | C = 1.000e-01 | score: 0.676 |\n",
      "\t| g = 2.069e-02 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 4.833e-02 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 1.129e-01 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 2.637e-01 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 6.158e-01 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 1.438e+00 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 3.360e+00 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 7.848e+00 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 1.833e+01 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 4.281e+01 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 1.000e+02 | C = 1.000e-05 | score: 0.551 |\n",
      "\tBest score was 0.701 with C = 1.00e-01 and g = 0.00379269019073225\n",
      "n = 50\n",
      "\t| g = 1.000e-05 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 2.336e-05 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 5.456e-05 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 1.274e-04 | C = 1.000e-01 | score: 0.562 |\n",
      "\t| g = 2.976e-04 | C = 1.000e-01 | score: 0.656 |\n",
      "\t| g = 6.952e-04 | C = 1.000e-01 | score: 0.671 |\n",
      "\t| g = 1.624e-03 | C = 1.000e-01 | score: 0.693 |\n",
      "\t| g = 3.793e-03 | C = 1.000e-01 | score: 0.699 |\n",
      "\t| g = 8.859e-03 | C = 1.000e-01 | score: 0.667 |\n",
      "\t| g = 2.069e-02 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 4.833e-02 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 1.129e-01 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 2.637e-01 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 6.158e-01 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 1.438e+00 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 3.360e+00 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 7.848e+00 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 1.833e+01 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 4.281e+01 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 1.000e+02 | C = 1.000e-05 | score: 0.551 |\n",
      "\tBest score was 0.699 with C = 1.00e-01 and g = 0.00379269019073225\n",
      "n = 70\n",
      "\t| g = 1.000e-05 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 2.336e-05 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 5.456e-05 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 1.274e-04 | C = 1.000e-01 | score: 0.562 |\n",
      "\t| g = 2.976e-04 | C = 1.000e-01 | score: 0.657 |\n",
      "\t| g = 6.952e-04 | C = 1.000e-01 | score: 0.670 |\n",
      "\t| g = 1.624e-03 | C = 1.000e-01 | score: 0.690 |\n",
      "\t| g = 3.793e-03 | C = 1.000e-01 | score: 0.699 |\n",
      "\t| g = 8.859e-03 | C = 1.000e-01 | score: 0.656 |\n",
      "\t| g = 2.069e-02 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 4.833e-02 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 1.129e-01 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 2.637e-01 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 6.158e-01 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 1.438e+00 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 3.360e+00 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 7.848e+00 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 1.833e+01 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 4.281e+01 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 1.000e+02 | C = 1.000e-05 | score: 0.551 |\n",
      "\tBest score was 0.699 with C = 1.00e-01 and g = 0.00379269019073225\n",
      "n = 97\n",
      "\t| g = 1.000e-05 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 2.336e-05 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 5.456e-05 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 1.274e-04 | C = 1.000e-01 | score: 0.561 |\n",
      "\t| g = 2.976e-04 | C = 1.000e-01 | score: 0.659 |\n",
      "\t| g = 6.952e-04 | C = 1.000e-01 | score: 0.670 |\n",
      "\t| g = 1.624e-03 | C = 1.000e-01 | score: 0.694 |\n",
      "\t| g = 3.793e-03 | C = 1.000e-01 | score: 0.696 |\n",
      "\t| g = 8.859e-03 | C = 1.000e-01 | score: 0.617 |\n",
      "\t| g = 2.069e-02 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 4.833e-02 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 1.129e-01 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 2.637e-01 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 6.158e-01 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 1.438e+00 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 3.360e+00 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 7.848e+00 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 1.833e+01 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 4.281e+01 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 1.000e+02 | C = 1.000e-05 | score: 0.551 |\n",
      "\tBest score was 0.696 with C = 1.00e-01 and g = 0.00379269019073225\n",
      "n = 135\n",
      "\t| g = 1.000e-05 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 2.336e-05 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 5.456e-05 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 1.274e-04 | C = 1.000e-01 | score: 0.561 |\n",
      "\t| g = 2.976e-04 | C = 1.000e-01 | score: 0.658 |\n",
      "\t| g = 6.952e-04 | C = 1.000e-01 | score: 0.670 |\n",
      "\t| g = 1.624e-03 | C = 1.000e-01 | score: 0.694 |\n",
      "\t| g = 3.793e-03 | C = 1.000e-01 | score: 0.693 |\n",
      "\t| g = 8.859e-03 | C = 1.000e-01 | score: 0.571 |\n",
      "\t| g = 2.069e-02 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 4.833e-02 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 1.129e-01 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 2.637e-01 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 6.158e-01 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 1.438e+00 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 3.360e+00 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 7.848e+00 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 1.833e+01 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 4.281e+01 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 1.000e+02 | C = 1.000e-05 | score: 0.551 |\n",
      "\tBest score was 0.694 with C = 1.00e-01 and g = 0.001623776739188721\n",
      "n = 187\n",
      "\t| g = 1.000e-05 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 2.336e-05 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 5.456e-05 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 1.274e-04 | C = 1.000e-01 | score: 0.561 |\n",
      "\t| g = 2.976e-04 | C = 1.000e-01 | score: 0.657 |\n",
      "\t| g = 6.952e-04 | C = 1.000e-01 | score: 0.673 |\n",
      "\t| g = 1.624e-03 | C = 1.000e-01 | score: 0.691 |\n",
      "\t| g = 3.793e-03 | C = 1.000e-01 | score: 0.689 |\n",
      "\t| g = 8.859e-03 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 2.069e-02 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 4.833e-02 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 1.129e-01 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 2.637e-01 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 6.158e-01 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 1.438e+00 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 3.360e+00 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 7.848e+00 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 1.833e+01 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 4.281e+01 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 1.000e+02 | C = 1.000e-05 | score: 0.551 |\n",
      "\tBest score was 0.691 with C = 1.00e-01 and g = 0.001623776739188721\n",
      "n = 259\n",
      "\t| g = 1.000e-05 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 2.336e-05 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 5.456e-05 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 1.274e-04 | C = 1.000e-01 | score: 0.560 |\n",
      "\t| g = 2.976e-04 | C = 1.000e-01 | score: 0.659 |\n",
      "\t| g = 6.952e-04 | C = 1.000e-01 | score: 0.672 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t| g = 1.624e-03 | C = 1.000e-01 | score: 0.690 |\n",
      "\t| g = 3.793e-03 | C = 1.000e-01 | score: 0.682 |\n",
      "\t| g = 8.859e-03 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 2.069e-02 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 4.833e-02 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 1.129e-01 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 2.637e-01 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 6.158e-01 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 1.438e+00 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 3.360e+00 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 7.848e+00 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 1.833e+01 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 4.281e+01 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 1.000e+02 | C = 1.000e-05 | score: 0.551 |\n",
      "\tBest score was 0.690 with C = 1.00e-01 and g = 0.001623776739188721\n",
      "n = 360\n",
      "\t| g = 1.000e-05 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 2.336e-05 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 5.456e-05 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 1.274e-04 | C = 1.000e-01 | score: 0.560 |\n",
      "\t| g = 2.976e-04 | C = 1.000e-01 | score: 0.658 |\n",
      "\t| g = 6.952e-04 | C = 1.000e-01 | score: 0.672 |\n",
      "\t| g = 1.624e-03 | C = 1.000e-01 | score: 0.688 |\n",
      "\t| g = 3.793e-03 | C = 1.000e-01 | score: 0.668 |\n",
      "\t| g = 8.859e-03 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 2.069e-02 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 4.833e-02 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 1.129e-01 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 2.637e-01 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 6.158e-01 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 1.438e+00 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 3.360e+00 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 7.848e+00 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 1.833e+01 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 4.281e+01 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 1.000e+02 | C = 1.000e-05 | score: 0.551 |\n",
      "\tBest score was 0.688 with C = 1.00e-01 and g = 0.001623776739188721\n",
      "n = 500\n",
      "\t| g = 1.000e-05 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 2.336e-05 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 5.456e-05 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 1.274e-04 | C = 1.000e-01 | score: 0.560 |\n",
      "\t| g = 2.976e-04 | C = 1.000e-01 | score: 0.654 |\n",
      "\t| g = 6.952e-04 | C = 1.000e-01 | score: 0.672 |\n",
      "\t| g = 1.624e-03 | C = 1.000e-01 | score: 0.684 |\n",
      "\t| g = 3.793e-03 | C = 1.000e-01 | score: 0.651 |\n",
      "\t| g = 8.859e-03 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 2.069e-02 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 4.833e-02 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 1.129e-01 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 2.637e-01 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 6.158e-01 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 1.438e+00 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 3.360e+00 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 7.848e+00 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 1.833e+01 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 4.281e+01 | C = 1.000e-05 | score: 0.551 |\n",
      "\t| g = 1.000e+02 | C = 1.000e-05 | score: 0.551 |\n",
      "\tBest score was 0.684 with C = 1.00e-01 and g = 0.001623776739188721\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(5, shuffle=True, random_state=0)\n",
    "C_vals = np.logspace(-5, -1, 10)\n",
    "n_vals = np.logspace(0, 2.699, 20)\n",
    "n_vals = list(sorted(set([ int(n) for n in n_vals ])))\n",
    "g_vals = np.logspace(-5, 2, 20)\n",
    "bestScore = 0\n",
    "bestN = 0\n",
    "bestC = 0\n",
    "bestG = 0\n",
    "\n",
    "for n in n_vals:\n",
    "    print(\"n =\", n)\n",
    "    all_scores = []\n",
    "    for g in g_vals:\n",
    "        c_scores = []\n",
    "        for C in C_vals:\n",
    "            scores = []\n",
    "            for train, test in kf.split(sscaledX_train):\n",
    "                SVM = SVC(C=C, kernel='rbf', gamma=g)\n",
    "                PCAn = PCA(n, random_state=1000).fit(sscaledX_train[train])\n",
    "                X_new = PCAn.transform(sscaledX_train[train])\n",
    "                X_val_new = PCAn.transform(sscaledX_train[test])\n",
    "\n",
    "                SVM.fit(X_new, y_train.iloc[train])\n",
    "                scores.append(SVM.score(X_val_new, y_train.iloc[test]))\n",
    "\n",
    "            c_scores.append(np.mean(scores))\n",
    "        bestC = np.argmax(c_scores)\n",
    "        all_scores.append( (C_vals[bestC], c_scores[bestC]) )\n",
    "        print(\"\\t| g = {0:2.3e} | C = {1:2.3e} | score: {2:.3f} |\".format(g, C_vals[bestC], c_scores[bestC]))\n",
    "\n",
    "    means = [ x[1] for x in all_scores ]\n",
    "    best = np.argmax(means)\n",
    "    if means[best] > bestScore:\n",
    "        bestScore = means[best]\n",
    "        bestG = g_vals[best]\n",
    "        bestN = n\n",
    "        bestC = all_scores[best][0]\n",
    "    print(\"\\tBest score was {0:.3f} with C = {1:3.2e} and g = {2}\".format(means[best], all_scores[best][0], g_vals[best]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 26 0.00379269019073225\n",
      "0.6990116801437556\n"
     ]
    }
   ],
   "source": [
    "print(bestC, bestN, bestG)\n",
    "SVM = SVC(C=0.1, kernel='rbf', gamma=bestG)\n",
    "PCAn = PCA(bestN, random_state=1000).fit(sscaledX_train)\n",
    "X_new = PCAn.transform(sscaledX_train)\n",
    "X_val_new = PCAn.transform(sscaledX_val)\n",
    "\n",
    "SVM.fit(X_new, y_train)\n",
    "\n",
    "print( SVM.score(X_val_new, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Scaled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7295597484276729"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVM = SVC()\n",
    "\n",
    "SVM.fit(sscaledX_train, y_train)\n",
    "\n",
    "SVM.score(sscaledX_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MinMax Scaled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6621743036837376"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVM = SVC()\n",
    "\n",
    "SVM.fit(mmscaledX_train, y_train)\n",
    "\n",
    "SVM.score(mmscaledX_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-scaled Data (original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5265049415992812"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVM = SVC()\n",
    "\n",
    "SVM.fit(X_train, y_train)\n",
    "\n",
    "SVM.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Log-Loss Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5417941742009064"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVM = SVC(probability=True)\n",
    "\n",
    "SVM.fit(sscaledX_train, y_train)\n",
    "\n",
    "pred_probs = SVM.predict_proba(sscaledX_val)\n",
    "log_loss(y_val, pred_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM = SVC(probability=True)\n",
    "SVM.fit(sscaledX_train, y_train)\n",
    "SVM.fit(sscaledX_val, y_val)\n",
    "pred_probabilities = SVM.predict_proba(sscaledX_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - I have trained here on both the training and validation set since we are now 'locking in' the model before its usage and we can now accept the validation set as more training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#ANSWER_TEXT#\n",
    "\n",
    "***Your answer goes here:***\n",
    "\n",
    "In my investigation I tried several different classifiers for the problem and found that the Support Vector Machine with default settings (using a radial basis function and 'auto', scaled value for gamma). I have put several comments throughout the notebook where they were most appropriate.\n",
    "\n",
    "Overall I think it is interesting to note the classification accuracy of the naive approach was fairly good despite its simplicity and Occam's Razor would have us lean towards simplicity - but the log loss metric shows us how frail the solution is compared to the others.\n",
    "\n",
    "As for the 3 other classifiers tested, with their optimal parameters they are all within 2% of each other for classification accuracy and within 0.04 on their log loss scores so there wasn't a lot in it. The stock Support Vector Machine parameters using a Radial Basis Function turned out to be the best despite extensive searching of the parameter space available.\n",
    "\n",
    "Also, as shown with the last few code cells, it was very important to scale the data before fitting it with the classifiers and failure to do so reduced the classification accuracy significantly. It was also interesting to see that different methods of scaling the data did have a notable difference on the classification accuracy too in some cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.86738171, 0.13261829\n",
      "0.78276702, 0.21723298\n",
      "0.35001523, 0.64998477\n",
      "0.22463571, 0.77536429\n",
      "0.42022753, 0.57977247\n",
      "0.10403671, 0.89596329\n",
      "0.16939613, 0.83060387\n",
      "0.07952146, 0.92047854\n",
      "0.81337666, 0.18662334\n",
      "0.74057932, 0.25942068\n",
      "0.52519288, 0.47480712\n",
      "0.72558377, 0.27441623\n",
      "0.64427108, 0.35572892\n",
      "0.78501169, 0.21498831\n",
      "0.07235127, 0.92764873\n",
      "0.72530584, 0.27469416\n",
      "0.18378427, 0.81621573\n",
      "0.54443355, 0.45556645\n",
      "0.65862188, 0.34137812\n",
      "0.40408594, 0.59591406\n",
      "0.85332191, 0.14667809\n",
      "0.34849820, 0.65150180\n",
      "0.66255133, 0.33744867\n",
      "0.80238268, 0.19761732\n",
      "0.77871957, 0.22128043\n",
      "0.06248887, 0.93751113\n",
      "0.23623449, 0.76376551\n",
      "0.61225622, 0.38774378\n",
      "0.18602313, 0.81397687\n",
      "0.60917355, 0.39082645\n",
      "0.83232922, 0.16767078\n",
      "0.55899239, 0.44100761\n",
      "0.84185367, 0.15814633\n",
      "0.65456474, 0.34543526\n",
      "0.70812872, 0.29187128\n",
      "0.29967989, 0.70032011\n",
      "0.08457504, 0.91542496\n",
      "0.41786863, 0.58213137\n",
      "0.76456021, 0.23543979\n",
      "0.68149514, 0.31850486\n",
      "0.76562013, 0.23437987\n",
      "0.73086381, 0.26913619\n",
      "0.83720632, 0.16279368\n",
      "0.86321186, 0.13678814\n",
      "0.16098964, 0.83901036\n",
      "0.73337266, 0.26662734\n",
      "0.63003366, 0.36996634\n",
      "0.83783773, 0.16216227\n",
      "0.44757727, 0.55242273\n",
      "0.88655781, 0.11344219\n",
      "0.73821279, 0.26178721\n",
      "0.80085739, 0.19914261\n",
      "0.24281827, 0.75718173\n",
      "0.39033820, 0.60966180\n",
      "0.16758523, 0.83241477\n",
      "0.39782197, 0.60217803\n",
      "0.76918040, 0.23081960\n",
      "0.54031931, 0.45968069\n",
      "0.81695609, 0.18304391\n",
      "0.18573118, 0.81426882\n",
      "0.76726274, 0.23273726\n",
      "0.71713367, 0.28286633\n",
      "0.84337470, 0.15662530\n",
      "0.25037248, 0.74962752\n",
      "0.54075711, 0.45924289\n",
      "0.27649451, 0.72350549\n",
      "0.62844029, 0.37155971\n",
      "0.92632762, 0.07367238\n",
      "0.55789890, 0.44210110\n",
      "0.83213610, 0.16786390\n",
      "0.47138879, 0.52861121\n",
      "0.58056461, 0.41943539\n",
      "0.04690388, 0.95309612\n",
      "0.73533996, 0.26466004\n",
      "0.76233324, 0.23766676\n",
      "0.79747929, 0.20252071\n",
      "0.83113311, 0.16886689\n",
      "0.35365514, 0.64634486\n",
      "0.62855653, 0.37144347\n",
      "0.35805622, 0.64194378\n",
      "0.70541720, 0.29458280\n",
      "0.67091802, 0.32908198\n",
      "0.24166486, 0.75833514\n",
      "0.27367812, 0.72632188\n",
      "0.81656464, 0.18343536\n",
      "0.11760503, 0.88239497\n",
      "0.55202608, 0.44797392\n",
      "0.75593998, 0.24406002\n",
      "0.40898103, 0.59101897\n",
      "0.46909567, 0.53090433\n",
      "0.72042860, 0.27957140\n",
      "0.24802429, 0.75197571\n",
      "0.56060794, 0.43939206\n",
      "0.18746561, 0.81253439\n",
      "0.33794426, 0.66205574\n",
      "0.72839617, 0.27160383\n",
      "0.87148080, 0.12851920\n",
      "0.43254262, 0.56745738\n",
      "0.32434177, 0.67565823\n",
      "0.14073641, 0.85926359\n",
      "0.90255926, 0.09744074\n",
      "0.35378515, 0.64621485\n",
      "0.91378823, 0.08621177\n",
      "0.55028089, 0.44971911\n",
      "0.17406396, 0.82593604\n",
      "0.71939514, 0.28060486\n",
      "0.19543952, 0.80456048\n",
      "0.40105357, 0.59894643\n",
      "0.11969386, 0.88030614\n",
      "0.74978125, 0.25021875\n",
      "0.20759814, 0.79240186\n",
      "0.27582265, 0.72417735\n",
      "0.42371115, 0.57628885\n",
      "0.66638921, 0.33361079\n",
      "0.21592157, 0.78407843\n",
      "0.69579325, 0.30420675\n",
      "0.48643817, 0.51356183\n",
      "0.39118282, 0.60881718\n",
      "0.67214655, 0.32785345\n",
      "0.09550216, 0.90449784\n",
      "0.71470626, 0.28529374\n",
      "0.28670492, 0.71329508\n",
      "0.90528505, 0.09471495\n",
      "0.82620547, 0.17379453\n",
      "0.65020876, 0.34979124\n",
      "0.03284202, 0.96715798\n",
      "0.47894620, 0.52105380\n",
      "0.45426144, 0.54573856\n",
      "0.66677232, 0.33322768\n",
      "0.30005306, 0.69994694\n",
      "0.85111487, 0.14888513\n",
      "0.55941414, 0.44058586\n",
      "0.54438528, 0.45561472\n",
      "0.57061958, 0.42938042\n",
      "0.66147220, 0.33852780\n",
      "0.84601833, 0.15398167\n",
      "0.85440214, 0.14559786\n",
      "0.77361254, 0.22638746\n",
      "0.79772655, 0.20227345\n",
      "0.84525274, 0.15474726\n",
      "0.22563720, 0.77436280\n",
      "0.05975110, 0.94024890\n",
      "0.21580880, 0.78419120\n",
      "0.80185087, 0.19814913\n",
      "0.86168870, 0.13831130\n",
      "0.57408622, 0.42591378\n",
      "0.58369786, 0.41630214\n",
      "0.14693905, 0.85306095\n",
      "0.71243823, 0.28756177\n",
      "0.82312052, 0.17687948\n",
      "0.30632143, 0.69367857\n",
      "0.14289383, 0.85710617\n",
      "0.84661094, 0.15338906\n",
      "0.67995000, 0.32005000\n",
      "0.65894469, 0.34105531\n",
      "0.42710017, 0.57289983\n",
      "0.43247347, 0.56752653\n",
      "0.16702985, 0.83297015\n",
      "0.25276929, 0.74723071\n",
      "0.41965630, 0.58034370\n",
      "0.41078280, 0.58921720\n",
      "0.19523744, 0.80476256\n",
      "0.58144987, 0.41855013\n",
      "0.50000000, 0.50000000\n",
      "0.89350841, 0.10649159\n",
      "0.91104647, 0.08895353\n",
      "0.64060003, 0.35939997\n",
      "0.30941237, 0.69058763\n",
      "0.88393426, 0.11606574\n",
      "0.38008871, 0.61991129\n",
      "0.87845307, 0.12154693\n",
      "0.86471508, 0.13528492\n",
      "0.53900496, 0.46099504\n",
      "0.63722268, 0.36277732\n",
      "0.24260182, 0.75739818\n",
      "0.78363587, 0.21636413\n",
      "0.29653968, 0.70346032\n",
      "0.43462149, 0.56537851\n",
      "0.54265339, 0.45734661\n",
      "0.80078261, 0.19921739\n",
      "0.10402707, 0.89597293\n",
      "0.94794831, 0.05205169\n",
      "0.81588983, 0.18411017\n",
      "0.66740737, 0.33259263\n",
      "0.17782299, 0.82217701\n",
      "0.21018749, 0.78981251\n",
      "0.64805778, 0.35194222\n",
      "0.27205483, 0.72794517\n",
      "0.76414295, 0.23585705\n",
      "0.64259405, 0.35740595\n",
      "0.69872905, 0.30127095\n",
      "0.20515637, 0.79484363\n",
      "0.21196617, 0.78803383\n",
      "0.24095234, 0.75904766\n",
      "0.77281331, 0.22718669\n",
      "0.60338762, 0.39661238\n",
      "0.85415538, 0.14584462\n",
      "0.07857762, 0.92142238\n",
      "0.42592944, 0.57407056\n",
      "0.26482974, 0.73517026\n",
      "0.27168992, 0.72831008\n",
      "0.73097517, 0.26902483\n",
      "0.15895995, 0.84104005\n",
      "0.07374812, 0.92625188\n",
      "0.75456264, 0.24543736\n",
      "0.46365611, 0.53634389\n",
      "0.48246739, 0.51753261\n",
      "0.75846379, 0.24153621\n",
      "0.86237887, 0.13762113\n",
      "0.60456830, 0.39543170\n",
      "0.07655617, 0.92344383\n",
      "0.26144576, 0.73855424\n",
      "0.72948067, 0.27051933\n",
      "0.81625136, 0.18374864\n",
      "0.28869345, 0.71130655\n",
      "0.32472172, 0.67527828\n",
      "0.18016862, 0.81983138\n",
      "0.24587176, 0.75412824\n",
      "0.77890304, 0.22109696\n",
      "0.84555099, 0.15444901\n",
      "0.77307577, 0.22692423\n",
      "0.57262612, 0.42737388\n",
      "0.87545935, 0.12454065\n",
      "0.85009708, 0.14990292\n",
      "0.50000000, 0.50000000\n",
      "0.19636098, 0.80363902\n",
      "0.45083425, 0.54916575\n",
      "0.53534137, 0.46465863\n",
      "0.60274932, 0.39725068\n",
      "0.54487114, 0.45512886\n",
      "0.50000000, 0.50000000\n",
      "0.81916250, 0.18083750\n",
      "0.74724783, 0.25275217\n",
      "0.47960776, 0.52039224\n",
      "0.18347224, 0.81652776\n",
      "0.64726807, 0.35273193\n",
      "0.31904223, 0.68095777\n",
      "0.85786204, 0.14213796\n",
      "0.79404684, 0.20595316\n",
      "0.51026317, 0.48973683\n",
      "0.41910670, 0.58089330\n",
      "0.55467039, 0.44532961\n",
      "0.20438876, 0.79561124\n",
      "0.53483971, 0.46516029\n",
      "0.54148043, 0.45851957\n",
      "0.87503430, 0.12496570\n",
      "0.62587990, 0.37412010\n",
      "0.78944113, 0.21055887\n",
      "0.81371473, 0.18628527\n",
      "0.51113439, 0.48886561\n",
      "0.54443274, 0.45556726\n",
      "0.57465032, 0.42534968\n",
      "0.75629476, 0.24370524\n",
      "0.56140530, 0.43859470\n",
      "0.75506902, 0.24493098\n",
      "0.12862873, 0.87137127\n",
      "0.10118728, 0.89881272\n",
      "0.51408966, 0.48591034\n",
      "0.10607055, 0.89392945\n",
      "0.83202674, 0.16797326\n",
      "0.52124832, 0.47875168\n",
      "0.92738262, 0.07261738\n",
      "0.84276643, 0.15723357\n",
      "0.56999508, 0.43000492\n",
      "0.51686678, 0.48313322\n",
      "0.87506453, 0.12493547\n",
      "0.83174940, 0.16825060\n",
      "0.70831926, 0.29168074\n",
      "0.81210817, 0.18789183\n",
      "0.43696248, 0.56303752\n",
      "0.90220278, 0.09779722\n",
      "0.78263068, 0.21736932\n",
      "0.66976436, 0.33023564\n",
      "0.36200641, 0.63799359\n",
      "0.64658655, 0.35341345\n",
      "0.51309629, 0.48690371\n",
      "0.54164355, 0.45835645\n",
      "0.29020166, 0.70979834\n",
      "0.32947354, 0.67052646\n",
      "0.75753013, 0.24246987\n",
      "0.75010458, 0.24989542\n",
      "0.56766493, 0.43233507\n",
      "0.74136110, 0.25863890\n",
      "0.64354141, 0.35645859\n",
      "0.89331378, 0.10668622\n",
      "0.39538017, 0.60461983\n",
      "0.46547465, 0.53452535\n",
      "0.90337111, 0.09662889\n",
      "0.35038247, 0.64961753\n",
      "0.48611148, 0.51388852\n",
      "0.85268300, 0.14731700\n",
      "0.60761390, 0.39238610\n",
      "0.12347378, 0.87652622\n",
      "0.24588253, 0.75411747\n",
      "0.85447528, 0.14552472\n",
      "0.31062241, 0.68937759\n",
      "0.92517309, 0.07482691\n",
      "0.59035509, 0.40964491\n",
      "0.26070125, 0.73929875\n",
      "0.76322442, 0.23677558\n",
      "0.54640329, 0.45359671\n",
      "0.86648775, 0.13351225\n",
      "0.80991327, 0.19008673\n",
      "0.76124501, 0.23875499\n",
      "0.18016519, 0.81983481\n",
      "0.50944570, 0.49055430\n",
      "0.90135489, 0.09864511\n",
      "0.38371818, 0.61628182\n",
      "0.41359371, 0.58640629\n",
      "0.48599794, 0.51400206\n",
      "0.56718897, 0.43281103\n",
      "0.40404960, 0.59595040\n",
      "0.83270524, 0.16729476\n",
      "0.66097150, 0.33902850\n",
      "0.80651773, 0.19348227\n",
      "0.39926135, 0.60073865\n",
      "0.25109075, 0.74890925\n",
      "0.63160697, 0.36839303\n",
      "0.26575051, 0.73424949\n",
      "0.20534821, 0.79465179\n",
      "0.42671100, 0.57328900\n",
      "0.22619381, 0.77380619\n",
      "0.19439303, 0.80560697\n",
      "0.63080243, 0.36919757\n",
      "0.75798798, 0.24201202\n",
      "0.83135643, 0.16864357\n",
      "0.59183648, 0.40816352\n",
      "0.34390918, 0.65609082\n",
      "0.66009975, 0.33990025\n",
      "0.81450263, 0.18549737\n",
      "0.86132271, 0.13867729\n",
      "0.81065317, 0.18934683\n",
      "0.51274085, 0.48725915\n",
      "0.69050478, 0.30949522\n",
      "0.39465760, 0.60534240\n",
      "0.22441224, 0.77558776\n",
      "0.78127823, 0.21872177\n",
      "0.62467554, 0.37532446\n",
      "0.35895137, 0.64104863\n",
      "0.66945768, 0.33054232\n",
      "0.59678840, 0.40321160\n",
      "0.86794849, 0.13205151\n",
      "0.15562052, 0.84437948\n",
      "0.34815696, 0.65184304\n",
      "0.17898383, 0.82101617\n",
      "0.10789323, 0.89210677\n",
      "0.57833180, 0.42166820\n",
      "0.78610871, 0.21389129\n",
      "0.31377304, 0.68622696\n",
      "0.24373669, 0.75626331\n",
      "0.26373703, 0.73626297\n",
      "0.31969422, 0.68030578\n",
      "0.28733513, 0.71266487\n",
      "0.21817065, 0.78182935\n",
      "0.15949010, 0.84050990\n",
      "0.81186085, 0.18813915\n",
      "0.32304971, 0.67695029\n",
      "0.83860438, 0.16139562\n",
      "0.21804105, 0.78195895\n",
      "0.06044279, 0.93955721\n",
      "0.48717767, 0.51282233\n",
      "0.79402533, 0.20597467\n",
      "0.46299379, 0.53700621\n",
      "0.81681880, 0.18318120\n",
      "0.38002399, 0.61997601\n",
      "0.84575287, 0.15424713\n",
      "0.51029571, 0.48970429\n",
      "0.25358228, 0.74641772\n",
      "0.12245467, 0.87754533\n",
      "0.84955318, 0.15044682\n",
      "0.08882801, 0.91117199\n",
      "0.34661460, 0.65338540\n",
      "0.40259337, 0.59740663\n",
      "0.59800310, 0.40199690\n",
      "0.32905355, 0.67094645\n",
      "0.44793279, 0.55206721\n",
      "0.16544617, 0.83455383\n",
      "0.25429436, 0.74570564\n",
      "0.89252839, 0.10747161\n",
      "0.48707384, 0.51292616\n",
      "0.87646091, 0.12353909\n",
      "0.55094337, 0.44905663\n",
      "0.72922603, 0.27077397\n",
      "0.83776931, 0.16223069\n",
      "0.87050565, 0.12949435\n",
      "0.46262430, 0.53737570\n",
      "0.86558906, 0.13441094\n",
      "0.33132478, 0.66867522\n",
      "0.64741217, 0.35258783\n",
      "0.86966623, 0.13033377\n",
      "0.15641651, 0.84358349\n",
      "0.88803584, 0.11196416\n",
      "0.17673352, 0.82326648\n",
      "0.43607351, 0.56392649\n",
      "0.20240701, 0.79759299\n",
      "0.84258740, 0.15741260\n",
      "0.71205341, 0.28794659\n",
      "0.71452960, 0.28547040\n",
      "0.55919406, 0.44080594\n",
      "0.86866550, 0.13133450\n",
      "0.86558847, 0.13441153\n",
      "0.22743950, 0.77256050\n",
      "0.28878519, 0.71121481\n",
      "0.86142092, 0.13857908\n",
      "0.22456531, 0.77543469\n",
      "0.77893947, 0.22106053\n",
      "0.12866373, 0.87133627\n",
      "0.86266602, 0.13733398\n",
      "0.68617072, 0.31382928\n",
      "0.14445277, 0.85554723\n",
      "0.57080974, 0.42919026\n",
      "0.37031767, 0.62968233\n",
      "0.70644189, 0.29355811\n",
      "0.88529531, 0.11470469\n",
      "0.15871976, 0.84128024\n",
      "0.87560188, 0.12439812\n",
      "0.52989969, 0.47010031\n",
      "0.48537008, 0.51462992\n",
      "0.10694569, 0.89305431\n",
      "0.62559292, 0.37440708\n",
      "0.29971101, 0.70028899\n",
      "0.86142218, 0.13857782\n",
      "0.57730405, 0.42269595\n",
      "0.43023357, 0.56976643\n",
      "0.73517604, 0.26482396\n",
      "0.64750286, 0.35249714\n",
      "0.82325615, 0.17674385\n",
      "0.91210080, 0.08789920\n",
      "0.90488002, 0.09511998\n",
      "0.81833601, 0.18166399\n",
      "0.88662692, 0.11337308\n",
      "0.74647289, 0.25352711\n",
      "0.27630373, 0.72369627\n",
      "0.37187287, 0.62812713\n",
      "0.88700120, 0.11299880\n",
      "0.84195222, 0.15804778\n",
      "0.23284187, 0.76715813\n",
      "0.24235775, 0.75764225\n",
      "0.25309448, 0.74690552\n",
      "0.80406041, 0.19593959\n",
      "0.84421169, 0.15578831\n",
      "0.17848818, 0.82151182\n",
      "0.59849253, 0.40150747\n",
      "0.87263177, 0.12736823\n",
      "0.27681586, 0.72318414\n",
      "0.32360368, 0.67639632\n",
      "0.47919218, 0.52080782\n",
      "0.35487814, 0.64512186\n",
      "0.70381883, 0.29618117\n",
      "0.87933464, 0.12066536\n",
      "0.61614741, 0.38385259\n",
      "0.64022025, 0.35977975\n",
      "0.65613967, 0.34386033\n",
      "0.93121090, 0.06878910\n",
      "0.30280374, 0.69719626\n",
      "0.74831011, 0.25168989\n",
      "0.20686416, 0.79313584\n",
      "0.66879812, 0.33120188\n",
      "0.63847696, 0.36152304\n",
      "0.13245693, 0.86754307\n",
      "0.85566761, 0.14433239\n",
      "0.43923487, 0.56076513\n",
      "0.31630318, 0.68369682\n",
      "0.22408537, 0.77591463\n",
      "0.53520359, 0.46479641\n",
      "0.35899547, 0.64100453\n",
      "0.48144398, 0.51855602\n",
      "0.40152931, 0.59847069\n",
      "0.83340989, 0.16659011\n",
      "0.75531063, 0.24468937\n",
      "0.47817003, 0.52182997\n",
      "0.72974118, 0.27025882\n",
      "0.73784648, 0.26215352\n",
      "0.40785140, 0.59214860\n",
      "0.74772812, 0.25227188\n",
      "0.10242273, 0.89757727\n",
      "0.47675606, 0.52324394\n",
      "0.74200043, 0.25799957\n",
      "0.50820509, 0.49179491\n",
      "0.77934948, 0.22065052\n",
      "0.14210107, 0.85789893\n",
      "0.75423153, 0.24576847\n",
      "0.45276672, 0.54723328\n",
      "0.69860032, 0.30139968\n",
      "0.74042350, 0.25957650\n",
      "0.71849835, 0.28150165\n",
      "0.79859775, 0.20140225\n",
      "0.75667777, 0.24332223\n",
      "0.56996457, 0.43003543\n",
      "0.48496968, 0.51503032\n",
      "0.68948045, 0.31051955\n",
      "0.56808507, 0.43191493\n",
      "0.34829607, 0.65170393\n",
      "0.82392529, 0.17607471\n",
      "0.11696009, 0.88303991\n",
      "0.84651164, 0.15348836\n",
      "0.52395652, 0.47604348\n",
      "0.91016413, 0.08983587\n",
      "0.85538672, 0.14461328\n",
      "0.81751201, 0.18248799\n",
      "0.69705177, 0.30294823\n",
      "0.45970519, 0.54029481\n",
      "0.48875278, 0.51124722\n",
      "0.66101662, 0.33898338\n",
      "0.04050721, 0.95949279\n",
      "0.87560098, 0.12439902\n",
      "0.48106333, 0.51893667\n",
      "0.70101307, 0.29898693\n",
      "0.61543761, 0.38456239\n",
      "0.69083225, 0.30916775\n",
      "0.34740977, 0.65259023\n",
      "0.44177949, 0.55822051\n",
      "0.81997627, 0.18002373\n",
      "0.85425456, 0.14574544\n",
      "0.48955834, 0.51044166\n",
      "0.44353939, 0.55646061\n",
      "0.78433734, 0.21566266\n",
      "0.86254383, 0.13745617\n",
      "0.66574599, 0.33425401\n",
      "0.61446731, 0.38553269\n",
      "0.82369123, 0.17630877\n",
      "0.28704892, 0.71295108\n",
      "0.55243838, 0.44756162\n",
      "0.26049192, 0.73950808\n",
      "0.69598356, 0.30401644\n",
      "0.41410200, 0.58589800\n",
      "0.21248694, 0.78751306\n",
      "0.51611733, 0.48388267\n",
      "0.06270707, 0.93729293\n",
      "0.80253391, 0.19746609\n",
      "0.65494785, 0.34505215\n",
      "0.22366157, 0.77633843\n",
      "0.60311064, 0.39688936\n",
      "0.51231796, 0.48768204\n",
      "0.36135254, 0.63864746\n",
      "0.68013578, 0.31986422\n",
      "0.43281324, 0.56718676\n",
      "0.50771427, 0.49228573\n",
      "0.26195295, 0.73804705\n",
      "0.86870259, 0.13129741\n",
      "0.68818303, 0.31181697\n",
      "0.58040102, 0.41959898\n",
      "0.86958960, 0.13041040\n",
      "0.61433675, 0.38566325\n",
      "0.77546387, 0.22453613\n",
      "0.24896596, 0.75103404\n",
      "0.39581386, 0.60418614\n",
      "0.39699954, 0.60300046\n",
      "0.08063901, 0.91936099\n",
      "0.84809347, 0.15190653\n",
      "0.86348831, 0.13651169\n",
      "0.68811446, 0.31188554\n",
      "0.84442250, 0.15557750\n",
      "0.13270848, 0.86729152\n",
      "0.83759752, 0.16240248\n",
      "0.77954563, 0.22045437\n",
      "0.77798295, 0.22201705\n",
      "0.13357488, 0.86642512\n",
      "0.24644105, 0.75355895\n",
      "0.05109139, 0.94890861\n",
      "0.53128030, 0.46871970\n",
      "0.67741215, 0.32258785\n",
      "0.60913896, 0.39086104\n",
      "0.25940965, 0.74059035\n",
      "0.23297056, 0.76702944\n",
      "0.71042516, 0.28957484\n",
      "0.19843075, 0.80156925\n",
      "0.06847306, 0.93152694\n",
      "0.09182167, 0.90817833\n",
      "0.67808917, 0.32191083\n",
      "0.37726101, 0.62273899\n",
      "0.63033076, 0.36966924\n",
      "0.58971443, 0.41028557\n",
      "0.68758437, 0.31241563\n",
      "0.13831666, 0.86168334\n",
      "0.62625746, 0.37374254\n",
      "0.61752180, 0.38247820\n",
      "0.40603227, 0.59396773\n",
      "0.65662523, 0.34337477\n",
      "0.24483200, 0.75516800\n",
      "0.63533568, 0.36466432\n",
      "0.24275948, 0.75724052\n",
      "0.66210864, 0.33789136\n",
      "0.28759917, 0.71240083\n",
      "0.71901958, 0.28098042\n",
      "0.78045432, 0.21954568\n",
      "0.63572514, 0.36427486\n",
      "0.69733361, 0.30266639\n",
      "0.74599138, 0.25400862\n",
      "0.71765597, 0.28234403\n",
      "0.83292347, 0.16707653\n",
      "0.31259094, 0.68740906\n",
      "0.70637533, 0.29362467\n",
      "0.89126739, 0.10873261\n",
      "0.50000000, 0.50000000\n",
      "0.14797143, 0.85202857\n",
      "0.68989028, 0.31010972\n",
      "0.42566236, 0.57433764\n",
      "0.76022349, 0.23977651\n",
      "0.86464539, 0.13535461\n",
      "0.85508486, 0.14491514\n",
      "0.93982137, 0.06017863\n",
      "0.32243705, 0.67756295\n",
      "0.43136085, 0.56863915\n",
      "0.57648936, 0.42351064\n",
      "0.45552642, 0.54447358\n",
      "0.69982509, 0.30017491\n",
      "0.76640253, 0.23359747\n",
      "0.30036716, 0.69963284\n",
      "0.44656335, 0.55343665\n",
      "0.50000000, 0.50000000\n",
      "0.74532402, 0.25467598\n",
      "0.52867618, 0.47132382\n",
      "0.76570080, 0.23429920\n",
      "0.70381159, 0.29618841\n",
      "0.45724436, 0.54275564\n",
      "0.45606576, 0.54393424\n",
      "0.93138903, 0.06861097\n",
      "0.79442732, 0.20557268\n",
      "0.40768249, 0.59231751\n",
      "0.59100351, 0.40899649\n",
      "0.68141356, 0.31858644\n",
      "0.38901353, 0.61098647\n",
      "0.48421826, 0.51578174\n",
      "0.44427014, 0.55572986\n",
      "0.57643347, 0.42356653\n",
      "0.12828364, 0.87171636\n",
      "0.76400420, 0.23599580\n",
      "0.45580030, 0.54419970\n",
      "0.92788270, 0.07211730\n",
      "0.22680057, 0.77319943\n",
      "0.40811075, 0.59188925\n",
      "0.50000000, 0.50000000\n",
      "0.81401255, 0.18598745\n",
      "0.33002004, 0.66997996\n",
      "0.26079235, 0.73920765\n",
      "0.64706420, 0.35293580\n",
      "0.42721581, 0.57278419\n",
      "0.72823517, 0.27176483\n",
      "0.27115203, 0.72884797\n",
      "0.88179342, 0.11820658\n",
      "0.20078357, 0.79921643\n",
      "0.26415619, 0.73584381\n",
      "0.83641012, 0.16358988\n",
      "0.76004441, 0.23995559\n",
      "0.19348086, 0.80651914\n",
      "0.63334033, 0.36665967\n",
      "0.61676635, 0.38323365\n",
      "0.71985562, 0.28014438\n",
      "0.84076743, 0.15923257\n",
      "0.84305243, 0.15694757\n",
      "0.44463891, 0.55536109\n",
      "0.79379553, 0.20620447\n",
      "0.78437711, 0.21562289\n",
      "0.08113901, 0.91886099\n",
      "0.81364048, 0.18635952\n",
      "0.50000000, 0.50000000\n",
      "0.18965217, 0.81034783\n",
      "0.41279566, 0.58720434\n",
      "0.38108052, 0.61891948\n",
      "0.15332091, 0.84667909\n",
      "0.55651261, 0.44348739\n",
      "0.78574970, 0.21425030\n",
      "0.59838240, 0.40161760\n",
      "0.31004340, 0.68995660\n",
      "0.85828136, 0.14171864\n",
      "0.48430240, 0.51569760\n",
      "0.37976189, 0.62023811\n",
      "0.70774993, 0.29225007\n",
      "0.83907430, 0.16092570\n",
      "0.70282535, 0.29717465\n",
      "0.26230569, 0.73769431\n",
      "0.22601658, 0.77398342\n",
      "0.86447459, 0.13552541\n",
      "0.36557053, 0.63442947\n",
      "0.57985270, 0.42014730\n",
      "0.77264183, 0.22735817\n",
      "0.43435299, 0.56564701\n",
      "0.38233537, 0.61766463\n",
      "0.74779500, 0.25220500\n",
      "0.75098121, 0.24901879\n",
      "0.37089344, 0.62910656\n",
      "0.21914752, 0.78085248\n",
      "0.54155994, 0.45844006\n",
      "0.53646602, 0.46353398\n",
      "0.62990576, 0.37009424\n",
      "0.33659031, 0.66340969\n",
      "0.38340066, 0.61659934\n",
      "0.08565095, 0.91434905\n",
      "0.22067811, 0.77932189\n",
      "0.74599113, 0.25400887\n",
      "0.51940028, 0.48059972\n",
      "0.79606141, 0.20393859\n",
      "0.40406805, 0.59593195\n",
      "0.59138141, 0.40861859\n",
      "0.57009747, 0.42990253\n",
      "0.84919537, 0.15080463\n",
      "0.64023465, 0.35976535\n",
      "0.63603386, 0.36396614\n",
      "0.17252773, 0.82747227\n",
      "0.74718188, 0.25281812\n",
      "0.57786653, 0.42213347\n",
      "0.32046210, 0.67953790\n",
      "0.86285671, 0.13714329\n",
      "0.26580432, 0.73419568\n",
      "0.77616435, 0.22383565\n",
      "0.24558149, 0.75441851\n",
      "0.07493492, 0.92506508\n",
      "0.74026920, 0.25973080\n",
      "0.81332450, 0.18667550\n",
      "0.86280178, 0.13719822\n",
      "0.77617890, 0.22382110\n",
      "0.72319326, 0.27680674\n",
      "0.81610050, 0.18389950\n",
      "0.57907348, 0.42092652\n",
      "0.87601898, 0.12398102\n",
      "0.07871081, 0.92128919\n",
      "0.34971159, 0.65028841\n",
      "0.46812605, 0.53187395\n",
      "0.45940605, 0.54059395\n",
      "0.69360327, 0.30639673\n",
      "0.43938125, 0.56061875\n",
      "0.79917849, 0.20082151\n",
      "0.31109797, 0.68890203\n",
      "0.51207924, 0.48792076\n",
      "0.56361819, 0.43638181\n",
      "0.51039392, 0.48960608\n",
      "0.91071450, 0.08928550\n",
      "0.63748554, 0.36251446\n",
      "0.44503400, 0.55496600\n",
      "0.40898529, 0.59101471\n",
      "0.75255341, 0.24744659\n",
      "0.43545666, 0.56454334\n",
      "0.03328850, 0.96671150\n",
      "0.77058300, 0.22941700\n",
      "0.44701508, 0.55298492\n",
      "0.71292860, 0.28707140\n",
      "0.86628668, 0.13371332\n",
      "0.44842406, 0.55157594\n",
      "0.73046709, 0.26953291\n",
      "0.85772948, 0.14227052\n",
      "0.71167012, 0.28832988\n",
      "0.70613316, 0.29386684\n",
      "0.43261062, 0.56738938\n",
      "0.82131198, 0.17868802\n",
      "0.42353534, 0.57646466\n",
      "0.26717650, 0.73282350\n",
      "0.16690268, 0.83309732\n",
      "0.16380873, 0.83619127\n",
      "0.79013474, 0.20986526\n",
      "0.81425805, 0.18574195\n",
      "0.45181208, 0.54818792\n",
      "0.22101502, 0.77898498\n",
      "0.54119578, 0.45880422\n",
      "0.69097643, 0.30902357\n",
      "0.85850718, 0.14149282\n",
      "0.59255933, 0.40744067\n",
      "0.59562647, 0.40437353\n",
      "0.75452448, 0.24547552\n",
      "0.35551633, 0.64448367\n",
      "0.23716544, 0.76283456\n",
      "0.16348199, 0.83651801\n",
      "0.09601255, 0.90398745\n",
      "0.58976091, 0.41023909\n",
      "0.57313161, 0.42686839\n",
      "0.60350126, 0.39649874\n",
      "0.28333607, 0.71666393\n",
      "0.79230479, 0.20769521\n",
      "0.65265666, 0.34734334\n",
      "0.19974878, 0.80025122\n",
      "0.93101656, 0.06898344\n",
      "0.44187290, 0.55812710\n",
      "0.64365669, 0.35634331\n",
      "0.05694983, 0.94305017\n",
      "0.71017202, 0.28982798\n",
      "0.50000000, 0.50000000\n",
      "0.36089291, 0.63910709\n",
      "0.48110094, 0.51889906\n",
      "0.26724613, 0.73275387\n",
      "0.23222752, 0.76777248\n",
      "0.81359265, 0.18640735\n",
      "0.82662338, 0.17337662\n",
      "0.44185223, 0.55814777\n",
      "0.88688291, 0.11311709\n",
      "0.75310106, 0.24689894\n",
      "0.52968144, 0.47031856\n",
      "0.75335034, 0.24664966\n",
      "0.70349387, 0.29650613\n",
      "0.33512135, 0.66487865\n",
      "0.74990617, 0.25009383\n",
      "0.38285708, 0.61714292\n",
      "0.30537494, 0.69462506\n",
      "0.07804007, 0.92195993\n",
      "0.63155642, 0.36844358\n",
      "0.61740517, 0.38259483\n",
      "0.85362208, 0.14637792\n",
      "0.88108984, 0.11891016\n",
      "0.18426259, 0.81573741\n",
      "0.50000000, 0.50000000\n",
      "0.72554482, 0.27445518\n",
      "0.09920113, 0.90079887\n",
      "0.57526022, 0.42473978\n",
      "0.16219068, 0.83780932\n",
      "0.35777358, 0.64222642\n",
      "0.88932005, 0.11067995\n",
      "0.84117103, 0.15882897\n",
      "0.77657572, 0.22342428\n",
      "0.64750259, 0.35249741\n",
      "0.81175199, 0.18824801\n",
      "0.82591982, 0.17408018\n",
      "0.80112419, 0.19887581\n",
      "0.75229659, 0.24770341\n",
      "0.65757775, 0.34242225\n",
      "0.09325920, 0.90674080\n",
      "0.60634457, 0.39365543\n",
      "0.11538859, 0.88461141\n",
      "0.28972249, 0.71027751\n",
      "0.86483769, 0.13516231\n",
      "0.56843608, 0.43156392\n",
      "0.37771047, 0.62228953\n",
      "0.55497288, 0.44502712\n",
      "0.20196359, 0.79803641\n",
      "0.81888032, 0.18111968\n",
      "0.56020998, 0.43979002\n",
      "0.66344173, 0.33655827\n",
      "0.86602168, 0.13397832\n",
      "0.20322784, 0.79677216\n",
      "0.28057535, 0.71942465\n",
      "0.79272147, 0.20727853\n",
      "0.59704995, 0.40295005\n",
      "0.44729846, 0.55270154\n",
      "0.86194858, 0.13805142\n",
      "0.70755409, 0.29244591\n",
      "0.60986689, 0.39013311\n",
      "0.58617393, 0.41382607\n",
      "0.66821093, 0.33178907\n",
      "0.44528122, 0.55471878\n",
      "0.86755946, 0.13244054\n",
      "0.72215434, 0.27784566\n",
      "0.40641277, 0.59358723\n",
      "0.47971752, 0.52028248\n",
      "0.78072531, 0.21927469\n",
      "0.82484098, 0.17515902\n",
      "0.47267990, 0.52732010\n",
      "0.80513704, 0.19486296\n",
      "0.73183811, 0.26816189\n",
      "0.54314895, 0.45685105\n",
      "0.05849297, 0.94150703\n",
      "0.50000000, 0.50000000\n",
      "0.77527239, 0.22472761\n",
      "0.25535240, 0.74464760\n",
      "0.69545431, 0.30454569\n",
      "0.44695984, 0.55304016\n",
      "0.27113067, 0.72886933\n",
      "0.77186818, 0.22813182\n",
      "0.33235037, 0.66764963\n",
      "0.82607991, 0.17392009\n",
      "0.10608138, 0.89391862\n",
      "0.88409075, 0.11590925\n",
      "0.35027914, 0.64972086\n",
      "0.86751967, 0.13248033\n",
      "0.49014698, 0.50985302\n",
      "0.90100692, 0.09899308\n",
      "0.47660663, 0.52339337\n",
      "0.70417008, 0.29582992\n",
      "0.16591966, 0.83408034\n",
      "0.35187995, 0.64812005\n",
      "0.48134147, 0.51865853\n",
      "0.72545024, 0.27454976\n",
      "0.72123843, 0.27876157\n",
      "0.65287967, 0.34712033\n",
      "0.19358653, 0.80641347\n",
      "0.73971403, 0.26028597\n",
      "0.57739564, 0.42260436\n",
      "0.66604262, 0.33395738\n",
      "0.84245747, 0.15754253\n",
      "0.67923744, 0.32076256\n",
      "0.37826619, 0.62173381\n",
      "0.76745407, 0.23254593\n",
      "0.41745446, 0.58254554\n",
      "0.10331168, 0.89668832\n",
      "0.43995012, 0.56004988\n",
      "0.81104002, 0.18895998\n",
      "0.79898667, 0.20101333\n",
      "0.20112051, 0.79887949\n",
      "0.78356875, 0.21643125\n",
      "0.71200410, 0.28799590\n",
      "0.32955181, 0.67044819\n",
      "0.73007458, 0.26992542\n",
      "0.65210967, 0.34789033\n",
      "0.77432234, 0.22567766\n",
      "0.62261872, 0.37738128\n",
      "0.36229372, 0.63770628\n",
      "0.58125023, 0.41874977\n",
      "0.74370979, 0.25629021\n",
      "0.54395927, 0.45604073\n",
      "0.54429650, 0.45570350\n",
      "0.25062357, 0.74937643\n",
      "0.80296564, 0.19703436\n",
      "0.56562000, 0.43438000\n",
      "0.64053832, 0.35946168\n",
      "0.50976982, 0.49023018\n",
      "0.54755791, 0.45244209\n",
      "0.74279371, 0.25720629\n",
      "0.39504406, 0.60495594\n",
      "0.85645774, 0.14354226\n",
      "0.21849724, 0.78150276\n",
      "0.47591677, 0.52408323\n",
      "0.50732655, 0.49267345\n",
      "0.08819694, 0.91180306\n",
      "0.89165436, 0.10834564\n",
      "0.81112160, 0.18887840\n",
      "0.63452876, 0.36547124\n",
      "0.14187765, 0.85812235\n",
      "0.27441369, 0.72558631\n",
      "0.47275951, 0.52724049\n",
      "0.28210407, 0.71789593\n",
      "0.77663796, 0.22336204\n",
      "0.09350476, 0.90649524\n",
      "0.55789650, 0.44210350\n",
      "0.24976038, 0.75023962\n",
      "0.59037149, 0.40962851\n",
      "0.14931464, 0.85068536\n",
      "0.38701150, 0.61298850\n",
      "0.50000000, 0.50000000\n",
      "0.38519052, 0.61480948\n",
      "0.20995862, 0.79004138\n",
      "0.77765442, 0.22234558\n",
      "0.78685711, 0.21314289\n",
      "0.28605917, 0.71394083\n",
      "0.58698752, 0.41301248\n",
      "0.56286184, 0.43713816\n",
      "0.69627034, 0.30372966\n",
      "0.47701849, 0.52298151\n",
      "0.10520205, 0.89479795\n",
      "0.32066648, 0.67933352\n",
      "0.60020018, 0.39979982\n",
      "0.82372793, 0.17627207\n",
      "0.26742401, 0.73257599\n",
      "0.56736413, 0.43263587\n",
      "0.34428083, 0.65571917\n",
      "0.17057904, 0.82942096\n",
      "0.82490944, 0.17509056\n",
      "0.13645237, 0.86354763\n",
      "0.76548754, 0.23451246\n",
      "0.09030536, 0.90969464\n",
      "0.81511407, 0.18488593\n",
      "0.46899102, 0.53100898\n",
      "0.68106262, 0.31893738\n",
      "0.35865394, 0.64134606\n",
      "0.08148460, 0.91851540\n",
      "0.86869621, 0.13130379\n",
      "0.46608839, 0.53391161\n",
      "0.26970634, 0.73029366\n",
      "0.36033542, 0.63966458\n",
      "0.54314002, 0.45685998\n",
      "0.42293323, 0.57706677\n",
      "0.12152569, 0.87847431\n",
      "0.89176856, 0.10823144\n",
      "0.46773410, 0.53226590\n",
      "0.58743818, 0.41256182\n",
      "0.85771856, 0.14228144\n",
      "0.48307206, 0.51692794\n",
      "0.15581761, 0.84418239\n",
      "0.47929604, 0.52070396\n",
      "0.69974426, 0.30025574\n",
      "0.75978324, 0.24021676\n",
      "0.86878657, 0.13121343\n",
      "0.68018299, 0.31981701\n",
      "0.32496278, 0.67503722\n",
      "0.83804432, 0.16195568\n",
      "0.84076608, 0.15923392\n",
      "0.54876828, 0.45123172\n",
      "0.85236173, 0.14763827\n",
      "0.55405814, 0.44594186\n",
      "0.77424950, 0.22575050\n",
      "0.25007059, 0.74992941\n",
      "0.17119051, 0.82880949\n",
      "0.66660378, 0.33339622\n",
      "0.09929843, 0.90070157\n",
      "0.71141532, 0.28858468\n",
      "0.53830706, 0.46169294\n",
      "0.30396674, 0.69603326\n",
      "0.66785236, 0.33214764\n",
      "0.40388005, 0.59611995\n",
      "0.34021016, 0.65978984\n",
      "0.67802618, 0.32197382\n",
      "0.46367409, 0.53632591\n",
      "0.07585142, 0.92414858\n",
      "0.36887785, 0.63112215\n",
      "0.18836949, 0.81163051\n",
      "0.56644067, 0.43355933\n",
      "0.55496716, 0.44503284\n",
      "0.87781156, 0.12218844\n",
      "0.17258514, 0.82741486\n",
      "0.15184106, 0.84815894\n",
      "0.74425881, 0.25574119\n",
      "0.68728324, 0.31271676\n",
      "0.89814692, 0.10185308\n",
      "0.33452992, 0.66547008\n",
      "0.90277188, 0.09722812\n",
      "0.14423342, 0.85576658\n",
      "0.75695931, 0.24304069\n",
      "0.41536288, 0.58463712\n",
      "0.81504337, 0.18495663\n",
      "0.14692363, 0.85307637\n",
      "0.81481058, 0.18518942\n",
      "0.60715806, 0.39284194\n",
      "0.71941862, 0.28058138\n",
      "0.15136499, 0.84863501\n",
      "0.82100522, 0.17899478\n",
      "0.71497667, 0.28502333\n",
      "0.81707191, 0.18292809\n",
      "0.29117041, 0.70882959\n",
      "0.41155793, 0.58844207\n",
      "0.48288258, 0.51711742\n",
      "0.57635549, 0.42364451\n",
      "0.50803132, 0.49196868\n",
      "0.53209292, 0.46790708\n",
      "0.86687125, 0.13312875\n",
      "0.73438100, 0.26561900\n",
      "0.03834527, 0.96165473\n",
      "0.81370505, 0.18629495\n",
      "0.70781821, 0.29218179\n",
      "0.46272370, 0.53727630\n",
      "0.77013779, 0.22986221\n",
      "0.52625516, 0.47374484\n",
      "0.64162584, 0.35837416\n",
      "0.84722562, 0.15277438\n",
      "0.87243468, 0.12756532\n",
      "0.81094110, 0.18905890\n",
      "0.66502179, 0.33497821\n",
      "0.48756941, 0.51243059\n",
      "0.75367895, 0.24632105\n",
      "0.90207957, 0.09792043\n",
      "0.63510725, 0.36489275\n",
      "0.46655047, 0.53344953\n",
      "0.46537582, 0.53462418\n",
      "0.73246391, 0.26753609\n",
      "0.80896630, 0.19103370\n",
      "0.55631110, 0.44368890\n",
      "0.74956410, 0.25043590\n",
      "0.60461966, 0.39538034\n",
      "0.14573433, 0.85426567\n",
      "0.89095414, 0.10904586\n",
      "0.74885361, 0.25114639\n",
      "0.34797286, 0.65202714\n",
      "0.64678020, 0.35321980\n",
      "0.18139794, 0.81860206\n",
      "0.17862295, 0.82137705\n",
      "0.76650438, 0.23349562\n",
      "0.76480572, 0.23519428\n",
      "0.34884890, 0.65115110\n",
      "0.32918759, 0.67081241\n",
      "0.47476208, 0.52523792\n",
      "0.30591712, 0.69408288\n",
      "0.74207899, 0.25792101\n",
      "0.51953271, 0.48046729\n",
      "0.88546120, 0.11453880\n",
      "0.83527042, 0.16472958\n",
      "0.65823322, 0.34176678\n",
      "0.43216199, 0.56783801\n",
      "0.81772509, 0.18227491\n",
      "0.66657107, 0.33342893\n",
      "0.84408026, 0.15591974\n",
      "0.52451839, 0.47548161\n",
      "0.24958393, 0.75041607\n",
      "0.55166240, 0.44833760\n",
      "0.73180022, 0.26819978\n",
      "0.53275196, 0.46724804\n",
      "0.78529857, 0.21470143\n",
      "0.70662740, 0.29337260\n",
      "0.15651328, 0.84348672\n",
      "0.73354919, 0.26645081\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03699746, 0.96300254\n",
      "0.28773079, 0.71226921\n",
      "0.66582750, 0.33417250\n",
      "0.24263543, 0.75736457\n",
      "0.86822223, 0.13177777\n",
      "0.32893892, 0.67106108\n",
      "0.56197990, 0.43802010\n",
      "0.32080048, 0.67919952\n",
      "0.31309920, 0.68690080\n",
      "0.88875395, 0.11124605\n",
      "0.52703185, 0.47296815\n",
      "0.62088248, 0.37911752\n",
      "0.50556291, 0.49443709\n",
      "0.21294413, 0.78705587\n",
      "0.89513225, 0.10486775\n",
      "0.68345340, 0.31654660\n",
      "0.54252262, 0.45747738\n",
      "0.84827106, 0.15172894\n",
      "0.49158106, 0.50841894\n",
      "0.82819670, 0.17180330\n",
      "0.33770585, 0.66229415\n",
      "0.46215433, 0.53784567\n",
      "0.44763557, 0.55236443\n",
      "0.27256682, 0.72743318\n",
      "0.92039702, 0.07960298\n",
      "0.66911988, 0.33088012\n",
      "0.06016724, 0.93983276\n",
      "0.18440480, 0.81559520\n",
      "0.78996731, 0.21003269\n",
      "0.21827971, 0.78172029\n",
      "0.68838047, 0.31161953\n",
      "0.24339489, 0.75660511\n",
      "0.88300124, 0.11699876\n",
      "0.60987836, 0.39012164\n",
      "0.54956874, 0.45043126\n",
      "0.49131476, 0.50868524\n",
      "0.50000000, 0.50000000\n",
      "0.64818708, 0.35181292\n",
      "0.40840920, 0.59159080\n",
      "0.53255304, 0.46744696\n"
     ]
    }
   ],
   "source": [
    "#ANSWER_PROB#\n",
    "# Run this cell when you are ready to submit your test-set probabilities. This cell will generate some\n",
    "# warning messages if something is not right: make sure to address them!\n",
    "if pred_probabilities.shape != (1114, 2):\n",
    "    print('Array is of incorrect shape. Rectify this before submitting.')\n",
    "elif (pred_probabilities.sum(axis=1) != 1.0).all():\n",
    "    print('Submitted values are not correct probabilities. Rectify this before submitting.')\n",
    "else:\n",
    "    for _prob in pred_probabilities:\n",
    "        print('{:.8f}, {:.8f}'.format(_prob[0], _prob[1]))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
